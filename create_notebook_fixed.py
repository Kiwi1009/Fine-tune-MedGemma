#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‰µå»ºä¿®æ­£ç‰ˆçš„ MedGemma 27B ä¸­æ–‡å¾®èª¿ Jupyter Notebook
"""

import json

# å‰µå»º notebook çµæ§‹
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MedGemma 27B ä¸­æ–‡å¾®èª¿æŒ‡å—\n\n",
                "## ğŸ¯ å°ˆæ¡ˆç›®æ¨™\n\n",
                "æœ¬è…³æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA (Low-Rank Adaptation) æŠ€è¡“åœ¨é†«ç™‚å•ç­”æ•¸æ“šä¸Šå¾®èª¿ MedGemma 27B å¤šæ¨¡æ…‹æ¨¡å‹ã€‚\n",
                "å°ˆæ¡ˆé‡å° GPU è¨“ç·´é€²è¡Œäº†å„ªåŒ–ï¼Œç‰¹åˆ¥é©åˆ RTX 4090 ç­‰é«˜ç«¯é¡¯å¡ã€‚\n\n",
                "## ğŸ“‹ ä¸»è¦åŠŸèƒ½\n\n",
                "- **GPU åŠ é€Ÿè¨“ç·´**: æ”¯æ´ CUDA GPU åŠ é€Ÿï¼Œå„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨\n",
                "- **LoRA å¾®èª¿**: é«˜æ•ˆåƒæ•¸é©æ‡‰ï¼Œç„¡éœ€å®Œæ•´æ¨¡å‹é‡è¨“ç·´\n",
                "- **é†«ç™‚å°ˆç”¨**: åŸºæ–¼ MedQuAD é†«ç™‚å•ç­”æ•¸æ“šé›†è¨“ç·´\n",
                "- **è¨˜æ†¶é«”ç›£æ§**: å…§å»º GPU è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤å’Œå„ªåŒ–å»ºè­°\n",
                "- **ä¸­æ–‡æ”¯æ´**: å®Œæ•´çš„ä¸­æ–‡èªªæ˜å’ŒéŒ¯èª¤è™•ç†\n\n",
                "## âš ï¸ é‡è¦æé†’\n\n",
                "âš ï¸ **é†«ç™‚å…è²¬è²æ˜**: æ­¤æ¨¡å‹åƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”ã€‚é†«ç™‚å»ºè­°æ‡‰ç”±åˆæ ¼é†«ç™‚å°ˆæ¥­äººå“¡é©—è­‰ã€‚"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ç’°å¢ƒè¨­ç½®èˆ‡å¥—ä»¶å®‰è£\n\n",
                "é¦–å…ˆå®‰è£æ‰€æœ‰å¿…è¦çš„å¥—ä»¶ä¸¦æª¢æŸ¥ GPU å¯ç”¨æ€§ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# å®‰è£å¿…è¦å¥—ä»¶\n",
                "%pip install -q torch transformers peft bitsandbytes datasets pandas accelerate huggingface_hub\n",
                "%pip install -q -U transformers==4.44.0\n\n",
                "print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# å°å…¥å¿…è¦å¥—ä»¶\n",
                "import torch\n",
                "import pandas as pd\n",
                "import os\n",
                "import warnings\n",
                "import json\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling,\n",
                "    pipeline\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, prepare_model_for_kbit_training\n",
                "from datasets import Dataset\n",
                "from huggingface_hub import login, HfApi\n\n",
                "warnings.filterwarnings('ignore')\n\n",
                "print(\"âœ… å¥—ä»¶å°å…¥å®Œæˆ!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. HuggingFace èªè­‰è¨­ç½®\n\n",
                "MedGemma æ˜¯ä¸€å€‹éœ€è¦æˆæ¬Šçš„æ¨¡å‹ï¼Œéœ€è¦å…ˆé€²è¡Œ HuggingFace èªè­‰ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ” MedGemma èªè­‰è¨­ç½®\n",
                "print(\"ğŸ” MedGemma èªè­‰è¨­ç½®\")\n",
                "print(\"=\" * 50)\n",
                "print(\"MedGemma æ˜¯ä¸€å€‹éœ€è¦æˆæ¬Šçš„æ¨¡å‹ï¼Œè«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œèªè­‰:\")\n",
                "print(\"1. å‰å¾€ https://huggingface.co/google/medgemma-27b-multimodal\")\n",
                "print(\"2. é»æ“Š 'Request Access' ä¸¦æ¥å—æˆæ¬Šå”è­°\")\n",
                "print(\"3. ç­‰å¾…å¯©æ ¸é€šé (é€šå¸¸éœ€è¦å¹¾åˆ†é˜åˆ°å¹¾å°æ™‚)\")\n",
                "print(\"4. å¾ https://huggingface.co/settings/tokens å–å¾—æ‚¨çš„ token\")\n",
                "print(\"5. ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€é€²è¡Œèªè­‰\")\n",
                "print(\"=\" * 50)\n\n",
                "# æ–¹æ³• 1: äº’å‹•å¼ç™»å…¥ (æ¨è–¦ - æœƒæç¤ºè¼¸å…¥ token)\n",
                "# from huggingface_hub import login\n",
                "# login()\n\n",
                "# æ–¹æ³• 2: ç›´æ¥ token ç™»å…¥ (æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš› token)\n",
                "# from huggingface_hub import login\n",
                "# login(token=\"hf_your_token_here\")\n\n",
                "# æ–¹æ³• 3: ç’°å¢ƒè®Šæ•¸ (å°‡ token è¨­ç‚ºç’°å¢ƒè®Šæ•¸)\n",
                "# import os\n",
                "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"hf_your_token_here\"\n\n",
                "print(\"\\nâš ï¸  é‡è¦: è«‹å–æ¶ˆè¨»è§£ä¸¦åŸ·è¡Œä¸Šè¿°èªè­‰æ–¹æ³•ä¹‹ä¸€!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU æª¢æŸ¥å’Œè¨­ç½®\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
                "    print(f\"CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
                "    \n",
                "    # æª¢æŸ¥è¨˜æ†¶é«”æ˜¯å¦è¶³å¤ \n",
                "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "    if gpu_memory < 20:\n",
                "        print(\"âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”ä¸è¶³ 20GBï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ‰¹æ¬¡å¤§å°\")\n",
                "    elif gpu_memory >= 24:\n",
                "        print(\"âœ… GPU è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨é è¨­è¨­ç½®\")\n",
                "    else:\n",
                "        print(\"âš¡ GPU è¨˜æ†¶é«”é©ä¸­ï¼Œå»ºè­°ç›£æ§ä½¿ç”¨æƒ…æ³\")\n",
                "else:\n",
                "    print(\"âŒ è­¦å‘Š: æœªæª¢æ¸¬åˆ° GPU! æ­¤è…³æœ¬éœ€è¦ GPU æ‰èƒ½é‹è¡Œã€‚\")\n",
                "    \n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. è¼‰å…¥å’Œé è™•ç†æ•¸æ“šé›†\n\n",
                "è¼‰å…¥ MedQuAD.csv æ•¸æ“šé›†ä¸¦æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤è·Ÿéš¨å¾®èª¿æ ¼å¼ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# è¼‰å…¥æ•¸æ“šé›†\n",
                "print(\"ğŸ“Š è¼‰å…¥ MedQuAD æ•¸æ“šé›†...\")\n",
                "df = pd.read_csv('medquad.csv')\n\n",
                "print(f\"æ•¸æ“šé›†å½¢ç‹€: {df.shape}\")\n",
                "print(f\"æ¬„ä½: {df.columns.tolist()}\")\n",
                "print(\"\\nå‰ 3 è¡Œæ•¸æ“š:\")\n",
                "print(df.head(3))\n\n",
                "print(\"\\nç¼ºå¤±å€¼çµ±è¨ˆ:\")\n",
                "print(df.isnull().sum())\n\n",
                "print(\"\\næ•¸æ“šä¾†æºçµ±è¨ˆ:\")\n",
                "print(df['source'].value_counts())\n",
                "print(\"\\nå°ˆæ³¨é ˜åŸŸçµ±è¨ˆ:\")\n",
                "print(df['focus_area'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æ ¼å¼åŒ–æ•¸æ“šç‚ºæŒ‡ä»¤-å›æ‡‰æ ¼å¼\n",
                "def format_instruction(row):\n",
                "    \"\"\"å°‡å•é¡Œå’Œç­”æ¡ˆæ ¼å¼åŒ–ç‚º MedGemma çš„æŒ‡ä»¤-å›æ‡‰æ ¼å¼\"\"\"\n",
                "    instruction = f\"Instruction: {row['question']}\\nResponse: {row['answer']}\"\n",
                "    return instruction\n\n",
                "df['text'] = df.apply(format_instruction, axis=1)\n\n",
                "# å–æ¨£æ•¸æ“šä»¥æ§åˆ¶è¨“ç·´æ™‚é–“ (å¯æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´)\n",
                "df_sample = df.sample(n=min(2000, len(df)), random_state=42)\n",
                "print(f\"ä½¿ç”¨ {len(df_sample)} å€‹æ¨£æœ¬é€²è¡Œå¾®èª¿\")\n\n",
                "print(\"\\næ ¼å¼åŒ–æ–‡æœ¬ç¯„ä¾‹:\")\n",
                "print(df_sample['text'].iloc[0][:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. é…ç½®æ¨¡å‹ (4-bit é‡åŒ–)\n\n",
                "è¼‰å…¥ MedGemma 27B ä¸¦ä½¿ç”¨ 4-bit é‡åŒ–ä»¥å„ªåŒ– RTX 4090 çš„è¨˜æ†¶é«”ä½¿ç”¨ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æ¨¡å‹ ID\n",
                "model_id = \"google/medgemma-27b-multimodal\"\n\n",
                "# æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦å·²èªè­‰\n",
                "try:\n",
                "    api = HfApi()\n",
                "    # å˜—è©¦ç²å–æ¨¡å‹è³‡è¨Š - å¦‚æœæœªèªè­‰æœƒå¤±æ•—\n",
                "    model_info = api.model_info(model_id)\n",
                "    print(\"âœ… èªè­‰æˆåŠŸ! æ¨¡å‹å­˜å–å·²ç¢ºèªã€‚\")\n",
                "except Exception as e:\n",
                "    print(\"âŒ èªè­‰å¤±æ•—!\")\n",
                "    print(f\"éŒ¯èª¤: {e}\")\n",
                "    print(\"\\nğŸ”§ å¿«é€Ÿä¿®å¾©:\")\n",
                "    print(\"åœ¨æ–°å–®å…ƒæ ¼ä¸­é‹è¡Œ:\")\n",
                "    print(\"from huggingface_hub import login\")\n",
                "    print(\"login()  # é€™æœƒæç¤ºæ‚¨è¼¸å…¥ token\")\n",
                "    print(\"\\næˆ–ç›´æ¥è¨­ç½®æ‚¨çš„ token:\")\n",
                "    print(\"login(token='your_huggingface_token_here')\")\n",
                "    print(\"\\nç„¶å¾Œé‡æ–°é‹è¡Œæ­¤å–®å…ƒæ ¼ã€‚\")\n",
                "    raise Exception(\"è«‹å…ˆé€²è¡Œ HuggingFace èªè­‰!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# é…ç½® 4-bit é‡åŒ–\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n\n",
                "print(\"è¼‰å…¥ tokenizer...\")\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    tokenizer.padding_side = \"right\"\n",
                "    print(\"âœ… Tokenizer è¼‰å…¥æˆåŠŸ!\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Tokenizer è¼‰å…¥å¤±æ•—: {e}\")\n",
                "    print(\"é€™é€šå¸¸è¡¨ç¤ºéœ€è¦èªè­‰ã€‚\")\n",
                "    raise e\n\n",
                "print(\"è¼‰å…¥æ¨¡å‹ (4-bit é‡åŒ–)...\")\n",
                "try:\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "    )\n",
                "    print(\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ!\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
                "    print(\"å¸¸è¦‹è§£æ±ºæ–¹æ¡ˆ:\")\n",
                "    print(\"1. ç¢ºèªæ‚¨å·²é€²è¡Œ HuggingFace èªè­‰\")\n",
                "    print(\"2. æª¢æŸ¥æ‚¨æ˜¯å¦æœ‰ MedGemma æ¨¡å‹çš„å­˜å–æ¬Šé™\")\n",
                "    print(\"3. é©—è­‰æ‚¨çš„ç¶²è·¯é€£ç·š\")\n",
                "    raise e\n\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"\\nğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³:\")\n",
                "    print(f\"  å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
                "    print(f\"  å·²ä¿ç•™: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. é…ç½® LoRA\n\n",
                "è¨­ç½® LoRA (Low-Rank Adaptation) åƒæ•¸ä»¥å•Ÿç”¨é«˜æ•ˆå¾®èª¿ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# é…ç½® LoRA\n",
                "lora_config = LoraConfig(\n",
                "    r=16,                    # LoRA ç§©\n",
                "    lora_alpha=32,           # Alpha åƒæ•¸\n",
                "    lora_dropout=0.05,       # Dropout ç‡\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"],  # ç›®æ¨™æ³¨æ„åŠ›æ¨¡çµ„\n",
                ")\n\n",
                "# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ä»¥ç¯€çœè¨˜æ†¶é«”\n",
                "model.gradient_checkpointing_enable()\n\n",
                "# æº–å‚™æ¨¡å‹é€²è¡Œ k-bit è¨“ç·´\n",
                "model = prepare_model_for_kbit_training(model)\n\n",
                "# ç²å– PEFT æ¨¡å‹\n",
                "model = get_peft_model(model, lora_config)\n\n",
                "# æ‰“å°å¯è¨“ç·´åƒæ•¸\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "all_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
                "print(f\"ç¸½åƒæ•¸: {all_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. æº–å‚™è¨“ç·´æ•¸æ“šé›†\n\n",
                "æ‰‹å‹•å°æ•¸æ“šé›†é€²è¡Œ tokenization ä¸¦æº–å‚™ç”¨æ–¼æ¨™æº– Trainer çš„è¨“ç·´ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# è½‰æ›ç‚º HuggingFace Dataset\n",
                "dataset = Dataset.from_pandas(df_sample[['text']])\n\n",
                "def tokenize_function(examples):\n",
                "    \"\"\"å°æ–‡æœ¬é€²è¡Œ tokenizationï¼Œé©ç”¨æ–¼å› æœèªè¨€å»ºæ¨¡\"\"\"\n",
                "    texts = [text + tokenizer.eos_token for text in examples['text']]\n",
                "    \n",
                "    model_inputs = tokenizer(\n",
                "        texts,\n",
                "        truncation=True,\n",
                "        padding=False,\n",
                "        max_length=512,  # å¯æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´\n",
                "        return_tensors=None\n",
                "    )\n",
                "    \n",
                "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
                "    \n",
                "    return model_inputs\n\n",
                "tokenized_dataset = dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names,\n",
                "    desc=\"Tokenizing æ•¸æ“šé›†\"\n",
                ")\n\n",
                "print(f\"Tokenized æ•¸æ“šé›†: {tokenized_dataset}\")\n",
                "print(f\"æ¨£æœ¬ tokenized ç¯„ä¾‹é•·åº¦: {len(tokenized_dataset[0]['input_ids'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. é…ç½®è¨“ç·´åƒæ•¸\n\n",
                "è¨­ç½®é‡å° RTX 4090 24GB VRAM å„ªåŒ–çš„è¨“ç·´åƒæ•¸ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# è¨“ç·´åƒæ•¸\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=4,  # å¯æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´\n",
                "    gradient_accumulation_steps=4,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=100,\n",
                "    logging_steps=25,\n",
                "    learning_rate=2e-5,\n",
                "    weight_decay=0.001,\n",
                "    bf16=True,  # ä½¿ç”¨ BF16 ç²¾åº¦\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=-1,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"tensorboard\",\n",
                "    save_total_limit=2,\n",
                "    dataloader_pin_memory=False,\n",
                "    remove_unused_columns=False,\n",
                ")\n\n",
                "print(\"è¨“ç·´åƒæ•¸é…ç½®:\")\n",
                "print(f\"æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}\")\n",
                "print(f\"æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {training_args.gradient_accumulation_steps}\")\n",
                "print(f\"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
                "print(f\"å­¸ç¿’ç‡: {training_args.learning_rate}\")\n",
                "print(f\"è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}\")\n",
                "print(f\"æ··åˆç²¾åº¦: BF16 = {training_args.bf16}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. åˆå§‹åŒ–æ•¸æ“šæ”¶é›†å™¨å’Œæ¨™æº– Trainer\n\n",
                "ä½¿ç”¨æ¨™æº– Trainer å’Œ DataCollatorForLanguageModeling é€²è¡Œå› æœèªè¨€å»ºæ¨¡ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# åˆå§‹åŒ–æ•¸æ“šæ”¶é›†å™¨\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False,  # ç¢ºä¿æˆ‘å€‘é€²è¡Œå› æœèªè¨€å»ºæ¨¡ï¼Œè€Œä¸æ˜¯æ©ç¢¼èªè¨€å»ºæ¨¡\n",
                "    pad_to_multiple_of=8  # å„ªåŒ–ç¾ä»£ GPU çš„å¼µé‡æ“ä½œ\n",
                ")\n\n",
                "# åˆå§‹åŒ– Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                ")\n\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"è¨“ç·´å‰ GPU è¨˜æ†¶é«”:\")\n",
                "    print(f\"  å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
                "    print(f\"  å·²ä¿ç•™: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
                "    print(f\"  å¯ç”¨: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n\n",
                "print(\"\\nTrainer å·²ä½¿ç”¨æ¨™æº– Trainer é¡åˆå§‹åŒ–\")\n",
                "print(\"æ³¨æ„: è¨“ç·´éœ€è¦ä¸€äº›æ™‚é–“ã€‚å¯åœ¨å¦ä¸€å€‹çµ‚ç«¯ä¸­ä½¿ç”¨ 'nvidia-smi' ç›£æ§ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ã€‚\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. é–‹å§‹è¨“ç·´\n\n",
                "ç¾åœ¨é–‹å§‹ä½¿ç”¨æ¨™æº– Trainer é€²è¡Œè¨“ç·´éç¨‹ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# é–‹å§‹è¨“ç·´\n",
                "try:\n",
                "    print(\"ğŸš€ é–‹å§‹è¨“ç·´...\")\n",
                "    trainer.train()\n",
                "    print(\"âœ… è¨“ç·´æˆåŠŸå®Œæˆ!\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ è¨“ç·´å¤±æ•—ï¼ŒéŒ¯èª¤: {e}\")\n",
                "    print(\"é€™å¯èƒ½æ˜¯ç”±æ–¼:\")\n",
                "    print(\"1. GPU è¨˜æ†¶é«”ä¸è¶³ - å˜—è©¦æ¸›å°‘ batch_size æˆ– max_length\")\n",
                "    print(\"2. æ¨¡å‹å­˜å–å•é¡Œ - ç¢ºä¿æ‚¨æœ‰ MedGemma æ¨¡å‹çš„å­˜å–æ¬Šé™\")\n",
                "    print(\"3. CUDA ç›¸å®¹æ€§å•é¡Œ - æª¢æŸ¥æ‚¨çš„ PyTorch å’Œ CUDA ç‰ˆæœ¬\")\n",
                "    \n",
                "if torch.cuda.is_available():\n",
                "    print(f\"\\nè¨“ç·´å¾Œ GPU è¨˜æ†¶é«”:\")\n",
                "    print(f\"  å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
                "    print(f\"  å·²ä¿ç•™: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. ä¿å­˜å¾®èª¿æ¨¡å‹\n\n",
                "ä¿å­˜ LoRA é©é…å™¨å’Œå®Œæ•´çš„å¾®èª¿æ¨¡å‹ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ä¿å­˜å¾®èª¿æ¨¡å‹\n",
                "output_dir = \"./finetuned_medgemma_27b\"\n\n",
                "model.save_pretrained(output_dir)\n",
                "tokenizer.save_pretrained(output_dir)\n\n",
                "print(f\"å¾®èª¿æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}\")\n",
                "print(\"å…§å®¹:\")\n",
                "for file in os.listdir(output_dir):\n",
                "    print(f\"  - {file}\")\n\n",
                "# ä¿å­˜è¨“ç·´é…ç½®\n",
                "training_config = {\n",
                "    \"model_id\": model_id,\n",
                "    \"trainer_type\": \"normal_trainer\",\n",
                "    \"lora_config\": {\n",
                "        \"r\": lora_config.r,\n",
                "        \"lora_alpha\": lora_config.lora_alpha,\n",
                "        \"lora_dropout\": lora_config.lora_dropout,\n",
                "        \"target_modules\": lora_config.target_modules,\n",
                "    },\n",
                "    \"training_args\": {\n",
                "        \"learning_rate\": training_args.learning_rate,\n",
                "        \"num_train_epochs\": training_args.num_train_epochs,\n",
                "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
                "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
                "    },\n",
                "    \"dataset_size\": len(tokenized_dataset),\n",
                "}\n\n",
                "with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
                "    json.dump(training_config, f, indent=2)\n\n",
                "print(\"\\nè¨“ç·´é…ç½®å·²ä¿å­˜åˆ° training_config.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. æ¸¬è©¦å¾®èª¿æ¨¡å‹\n\n",
                "é€šéåœ¨æ•¸æ“šé›†çš„æ¨£æœ¬å•é¡Œä¸Šé‹è¡Œæ¨ç†ä¾†æ¸¬è©¦å¾®èª¿æ¨¡å‹ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
                "torch.cuda.empty_cache()\n\n",
                "print(\"è¼‰å…¥å¾®èª¿æ¨¡å‹é€²è¡Œæ¨ç†...\")\n\n",
                "# è¼‰å…¥åŸºç¤æ¨¡å‹\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    torch_dtype=torch.bfloat16,\n",
                ")\n\n",
                "# è¼‰å…¥å¾®èª¿çš„ LoRA é©é…å™¨\n",
                "finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n\n",
                "# è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼\n",
                "finetuned_model.eval()\n\n",
                "print(\"å¾®èª¿æ¨¡å‹è¼‰å…¥æˆåŠŸ!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æ¸¬è©¦æ¨£æœ¬å•é¡Œ\n",
                "sample_question = df_sample['question'].iloc[0]\n",
                "print(f\"æ¨£æœ¬å•é¡Œ: {sample_question}\")\n",
                "print(\"\\n\" + \"=\"*50)\n\n",
                "test_input = f\"Instruction: {sample_question}\\nResponse:\"\n\n",
                "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n\n",
                "print(\"ç”Ÿæˆå›æ‡‰...\")\n",
                "with torch.no_grad():\n",
                "    outputs = finetuned_model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=256,\n",
                "        do_sample=True,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        pad_token_id=tokenizer.eos_token_id,\n",
                "        eos_token_id=tokenizer.eos_token_id,\n",
                "    )\n\n",
                "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n",
                "if \"Response:\" in response:\n",
                "    generated_response = response.split(\"Response:\")[-1].strip()\n",
                "else:\n",
                "    generated_response = response\n\n",
                "print(f\"ç”Ÿæˆå›æ‡‰: {generated_response}\")\n",
                "print(\"\\n\" + \"=\"*50)\n\n",
                "original_answer = df_sample[df_sample['question'] == sample_question]['answer'].iloc[0]\n",
                "print(f\"åŸå§‹ç­”æ¡ˆ: {original_answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§\n\n",
                "æª¢æŸ¥æœ€çµ‚çš„ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ä¸¦æä¾›å„ªåŒ–å»ºè­°ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# æª¢æŸ¥æœ€çµ‚ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\n",
                "if torch.cuda.is_available():\n",
                "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
                "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
                "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "    \n",
                "    print(\"æœ€çµ‚ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³:\")\n",
                "    print(f\"  å·²åˆ†é…: {allocated:.2f} GB\")\n",
                "    print(f\"  å·²ä¿ç•™: {reserved:.2f} GB\")\n",
                "    print(f\"  ç¸½ GPU è¨˜æ†¶é«”: {total:.2f} GB\")\n",
                "    print(f\"  è¨˜æ†¶é«”åˆ©ç”¨ç‡: {(reserved/total)*100:.1f}%\")\n",
                "    \n",
                "    print(\"\\nè¨˜æ†¶é«”å„ªåŒ–å»ºè­°:\")\n",
                "    if reserved > 20:\n",
                "        print(\"âš ï¸  æª¢æ¸¬åˆ°é«˜è¨˜æ†¶é«”ä½¿ç”¨ (>20GB)ã€‚å»ºè­°:\")\n",
                "        print(\"   - å°‡ batch_size å¾ 4 æ¸›å°‘åˆ° 2\")\n",
                "        print(\"   - å°‡ max_length å¾ 512 æ¸›å°‘åˆ° 256\")\n",
                "        print(\"   - ä½¿ç”¨ gradient_accumulation_steps=8 ä»¥ç¶­æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°\")\n",
                "    elif reserved > 15:\n",
                "        print(\"âš¡ è‰¯å¥½çš„è¨˜æ†¶é«”ä½¿ç”¨ (15-20GB)ã€‚ç•¶å‰è¨­ç½®æ˜¯æœ€ä½³çš„ã€‚\")\n",
                "    else:\n",
                "        print(\"âœ… ä½è¨˜æ†¶é«”ä½¿ç”¨ (<15GB)ã€‚æ‚¨å¯ä»¥:\")\n",
                "        print(\"   - å°‡ batch_size å¢åŠ åˆ° 8 ä»¥åŠ å¿«è¨“ç·´\")\n",
                "        print(\"   - å°‡ max_length å¢åŠ åˆ° 1024 ä»¥è™•ç†æ›´é•·åºåˆ—\")\n",
                "        print(\"   - ä½¿ç”¨æ›´å¤§çš„æ•¸æ“šé›†æ¨£æœ¬\")\n",
                "        \n",
                "    print(\"\\nå°æ–¼å³æ™‚ç›£æ§ï¼Œåœ¨çµ‚ç«¯ä¸­é‹è¡Œæ­¤å‘½ä»¤:\")\n",
                "    print(\"nvidia-smi -l 1\")\n",
                "    \n",
                "else:\n",
                "    print(\"æœªæª¢æ¸¬åˆ° GPUã€‚æ­¤è…³æœ¬éœ€è¦ CUDA ç›¸å®¹çš„ GPUã€‚\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. é¡å¤–æ¸¬è©¦åŠŸèƒ½\n\n",
                "é€™è£¡æœ‰ä¸€äº›ç”¨æ–¼ä½¿ç”¨è‡ªå®šç¾©å•é¡Œæ¸¬è©¦æ¨¡å‹çš„é¡å¤–åŠŸèƒ½ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_medical_response(question, model=finetuned_model, tokenizer=tokenizer, max_length=256):\n",
                "    \"\"\"\n",
                "    ä½¿ç”¨å¾®èª¿æ¨¡å‹ç‚ºçµ¦å®šå•é¡Œç”Ÿæˆé†«ç™‚å›æ‡‰ã€‚\n",
                "    \n",
                "    åƒæ•¸:\n",
                "        question (str): è¦å›ç­”çš„é†«ç™‚å•é¡Œ\n",
                "        model: å¾®èª¿æ¨¡å‹\n",
                "        tokenizer: tokenizer\n",
                "        max_length (int): å›æ‡‰çš„æœ€å¤§é•·åº¦\n",
                "    \n",
                "    è¿”å›:\n",
                "        str: ç”Ÿæˆçš„é†«ç™‚å›æ‡‰\n",
                "    \"\"\"\n",
                "    test_input = f\"Instruction: {question}\\nResponse:\"\n",
                "    \n",
                "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_length,\n",
                "            do_sample=True,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "            eos_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    if \"Response:\" in response:\n",
                "        generated_response = response.split(\"Response:\")[-1].strip()\n",
                "    else:\n",
                "        generated_response = response\n",
                "    \n",
                "    return generated_response\n\n",
                "# æ¸¬è©¦è‡ªå®šç¾©å•é¡Œ\n",
                "test_questions = [\n",
                "    \"ç³–å°¿ç—…çš„ç—‡ç‹€æ˜¯ä»€éº¼?\",\n",
                "    \"é«˜è¡€å£“å¦‚ä½•æ²»ç™‚?\",\n",
                "    \"å¿ƒè‡Ÿç—…çš„åŸå› æ˜¯ä»€éº¼?\",\n",
                "]\n\n",
                "print(\"ä½¿ç”¨è‡ªå®šç¾©å•é¡Œé€²è¡Œæ¸¬è©¦:\")\n",
                "print(\"=\"*60)\n\n",
                "for i, question in enumerate(test_questions, 1):\n",
                "    print(f\"\\næ¸¬è©¦ {i}:\")\n",
                "    print(f\"å•é¡Œ: {question}\")\n",
                "    response = generate_medical_response(question)\n",
                "    print(f\"å›æ‡‰: {response}\")\n",
                "    print(\"-\" * 40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ‰ æ¨™æº– TRAINER å®Œæˆ!\n\n",
                "print(\"âœ… æ¨™æº– Trainer å¯¦ç¾å®Œæˆ!\")\n",
                "print(\"=\" * 50)\n",
                "print(\"ğŸ“‹ æ‚¨å·²å®Œæˆçš„å·¥ä½œ:\")\n",
                "print(\"1. âœ… ç’°å¢ƒè¨­ç½®å’Œèªè­‰\")\n",
                "print(\"2. âœ… æ•¸æ“šé›†è¼‰å…¥å’Œé è™•ç†\")\n",
                "print(\"3. âœ… å…·æœ‰ 4-bit é‡åŒ–çš„æ¨¡å‹é…ç½®\")\n",
                "print(\"4. âœ… LoRA é…ç½®\")\n",
                "print(\"5. âœ… æ¨™æº– Trainer çš„æ‰‹å‹• tokenization\")\n",
                "print(\"6. âœ… è¨“ç·´åƒæ•¸å„ªåŒ–\")\n",
                "print(\"7. âœ… æ•¸æ“šæ”¶é›†å™¨å’Œ trainer åˆå§‹åŒ–\")\n",
                "print(\"8. âœ… è¨“ç·´åŸ·è¡Œ\")\n",
                "print(\"9. âœ… æ¨¡å‹ä¿å­˜\")\n",
                "print(\"10. âœ… æ¨ç†æ¸¬è©¦\")\n",
                "print(\"=\" * 50)\n\n",
                "print(\"\\nğŸ”§ å¦‚æœæ‚¨é‡åˆ° 'unauthorized' éŒ¯èª¤:\")\n",
                "print(\"1. é‹è¡Œèªè­‰å–®å…ƒæ ¼ (å–®å…ƒæ ¼ 2)\")\n",
                "print(\"2. æŒ‰ç…§èªè­‰æ­¥é©Ÿæ“ä½œ\")\n",
                "print(\"3. é‡æ–°é‹è¡Œæ¨¡å‹è¼‰å…¥å–®å…ƒæ ¼ (å–®å…ƒæ ¼ 4)\")\n",
                "print(\"4. ç¹¼çºŒå…¶é¤˜çš„è¨“ç·´\")\n\n",
                "print(\"\\nğŸ’¡ æ¨™æº– Trainer æ–¹æ³•æä¾›:\")\n",
                "print(\"- å° tokenization çš„å®Œæ•´æ§åˆ¶\")\n",
                "print(\"- æ‰‹å‹•æ•¸æ“šé›†é è™•ç†\")\n",
                "print(\"- æ˜ç¢ºçš„æ¨™ç±¤è™•ç†\")\n",
                "print(\"- æ›´å¥½çš„èª¿è©¦èƒ½åŠ›\")\n",
                "print(\"- æ›´å¤šè‡ªå®šç¾©è¨“ç·´çš„éˆæ´»æ€§\")\n\n",
                "print(\"\\nğŸš€ æº–å‚™å¥½è¨“ç·´æ‚¨çš„ MedGemma 27B æ¨¡å‹!\")\n",
                "print(\"æ‰€æœ‰ SFTTrainer å–®å…ƒæ ¼å·²è¢«ç§»é™¤ä»¥ä¿æŒæ¸…æ™°ã€‚\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# å¯«å…¥æ–‡ä»¶
with open('finetune_medgemma_27b_ä¸­æ–‡.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=2)

print('âœ… ä¿®æ­£ç‰ˆ Jupyter Notebook å‰µå»ºæˆåŠŸ!')
print('æ–‡ä»¶: finetune_medgemma_27b_ä¸­æ–‡.ipynb')
print('æ‰€æœ‰ 4B æ¨™è¨»å·²ä¿®æ­£ç‚º 27B') 