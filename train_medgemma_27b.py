#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MedGemma 27B ä¸­æ–‡å¾®èª¿è¨“ç·´è…³æœ¬

é€™å€‹è…³æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA (Low-Rank Adaptation) æŠ€è¡“åœ¨é†«ç™‚å•ç­”æ•¸æ“šä¸Šå¾®èª¿ MedGemma 27B å¤šæ¨¡æ…‹æ¨¡å‹ã€‚
å°ˆæ¡ˆé‡å° GPU è¨“ç·´é€²è¡Œäº†å„ªåŒ–ï¼Œç‰¹åˆ¥é©åˆ RTX 4090 ç­‰é«˜ç«¯é¡¯å¡ã€‚

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2024
"""

import os
import sys
import json
import warnings
import argparse
from pathlib import Path

import torch
import pandas as pd
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    TaskType, 
    PeftModel, 
    prepare_model_for_kbit_training
)
from datasets import Dataset
from huggingface_hub import login, HfApi

warnings.filterwarnings('ignore')

class MedGemmaTrainer:
    """MedGemma è¨“ç·´å™¨é¡"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            config (dict): é…ç½®å­—å…¸
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        print("ğŸš€ MedGemma 27B ä¸­æ–‡å¾®èª¿è¨“ç·´å™¨åˆå§‹åŒ–")
        print("=" * 60)
        
    def check_gpu(self):
        """æª¢æŸ¥ GPU å¯ç”¨æ€§å’Œè¨˜æ†¶é«”"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ… GPU å¯ç”¨: {gpu_name}")
            print(f"GPU è¨˜æ†¶é«”: {gpu_memory:.2f} GB")
            
            if gpu_memory < 20:
                print("âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”ä¸è¶³ 20GBï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ‰¹æ¬¡å¤§å°")
            elif gpu_memory >= 24:
                print("âœ… GPU è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨é è¨­è¨­ç½®")
            else:
                print("âš¡ GPU è¨˜æ†¶é«”é©ä¸­ï¼Œå»ºè­°ç›£æ§ä½¿ç”¨æƒ…æ³")
        else:
            print("âŒ éŒ¯èª¤: æœªæª¢æ¸¬åˆ° GPU! æ­¤è…³æœ¬éœ€è¦ CUDA ç›¸å®¹çš„ GPUã€‚")
            sys.exit(1)
            
    def authenticate_huggingface(self):
        """HuggingFace èªè­‰"""
        print("\nğŸ” é€²è¡Œ HuggingFace èªè­‰...")
        
        # æª¢æŸ¥æ˜¯å¦å·²è¨­ç½®ç’°å¢ƒè®Šæ•¸
        if os.getenv("HUGGINGFACE_HUB_TOKEN"):
            print("âœ… ç™¼ç¾ç’°å¢ƒè®Šæ•¸ä¸­çš„ HuggingFace token")
            return True
            
        # å˜—è©¦äº’å‹•å¼ç™»å…¥
        try:
            login()
            print("âœ… HuggingFace èªè­‰æˆåŠŸ!")
            return True
        except Exception as e:
            print(f"âŒ èªè­‰å¤±æ•—: {e}")
            print("\nğŸ”§ è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œèªè­‰:")
            print("1. å‰å¾€ https://huggingface.co/google/medgemma-4b-multimodal")
            print("2. é»æ“Š 'Request Access' ä¸¦æ¥å—æˆæ¬Šå”è­°")
            print("3. ç­‰å¾…å¯©æ ¸é€šé")
            print("4. è¨­ç½®ç’°å¢ƒè®Šæ•¸: export HUGGINGFACE_HUB_TOKEN='your_token'")
            print("5. é‡æ–°é‹è¡Œæ­¤è…³æœ¬")
            return False
            
    def load_dataset(self):
        """è¼‰å…¥å’Œé è™•ç†æ•¸æ“šé›†"""
        print("\nğŸ“Š è¼‰å…¥ MedQuAD æ•¸æ“šé›†...")
        
        try:
            df = pd.read_csv(self.config['data_path'])
            print(f"æ•¸æ“šé›†å½¢ç‹€: {df.shape}")
            print(f"æ¬„ä½: {df.columns.tolist()}")
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_columns = ['question', 'answer']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"æ•¸æ“šé›†ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                
            # æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤-å›æ‡‰æ ¼å¼
            def format_instruction(row):
                return f"Instruction: {row['question']}\nResponse: {row['answer']}"
                
            df['text'] = df.apply(format_instruction, axis=1)
            
            # å–æ¨£æ•¸æ“šä»¥æ§åˆ¶è¨“ç·´æ™‚é–“
            sample_size = min(self.config.get('sample_size', 2000), len(df))
            df_sample = df.sample(n=sample_size, random_state=42)
            print(f"ä½¿ç”¨ {len(df_sample)} å€‹æ¨£æœ¬é€²è¡Œå¾®èª¿")
            
            # è½‰æ›ç‚º HuggingFace Dataset
            self.dataset = Dataset.from_pandas(df_sample[['text']])
            print("âœ… æ•¸æ“šé›†è¼‰å…¥æˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šé›†è¼‰å…¥å¤±æ•—: {e}")
            raise
            
    def setup_model(self):
        """è¨­ç½®æ¨¡å‹å’Œ tokenizer"""
        print("\nğŸ¤– è¨­ç½® MedGemma æ¨¡å‹...")
        
        model_id = self.config['model_id']
        
        # æª¢æŸ¥æ¨¡å‹å­˜å–æ¬Šé™
        try:
            api = HfApi()
            model_info = api.model_info(model_id)
            print("âœ… æ¨¡å‹å­˜å–æ¬Šé™ç¢ºèª")
        except Exception as e:
            print(f"âŒ æ¨¡å‹å­˜å–å¤±æ•—: {e}")
            print("è«‹ç¢ºä¿æ‚¨å·²ç”³è«‹ MedGemma æ¨¡å‹å­˜å–æ¬Šé™")
            raise
            
        # é…ç½® 4-bit é‡åŒ–
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # è¼‰å…¥ tokenizer
        print("è¼‰å…¥ tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        
        # è¼‰å…¥æ¨¡å‹
        print("è¼‰å…¥æ¨¡å‹ (4-bit é‡åŒ–)...")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
        
        # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"GPU è¨˜æ†¶é«”ä½¿ç”¨: å·²åˆ†é… {allocated:.2f} GB, å·²ä¿ç•™ {reserved:.2f} GB")
            
    def setup_lora(self):
        """è¨­ç½® LoRA é…ç½®"""
        print("\nğŸ›ï¸  è¨­ç½® LoRA é…ç½®...")
        
        lora_config = LoraConfig(
            r=self.config.get('lora_r', 16),
            lora_alpha=self.config.get('lora_alpha', 32),
            lora_dropout=self.config.get('lora_dropout', 0.05),
            bias="none",
            task_type=TaskType.CAUSAL_LM,
            target_modules=self.config.get('target_modules', ["q_proj", "v_proj"]),
        )
        
        # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ä»¥ç¯€çœè¨˜æ†¶é«”
        self.model.gradient_checkpointing_enable()
        
        # æº–å‚™æ¨¡å‹é€²è¡Œ k-bit è¨“ç·´
        self.model = prepare_model_for_kbit_training(self.model)
        
        # ç²å– PEFT æ¨¡å‹
        self.model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è¨“ç·´åƒæ•¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in self.model.parameters())
        print(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)")
        print(f"ç¸½åƒæ•¸: {all_params:,}")
        
    def prepare_dataset(self):
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        print("\nğŸ“ æº–å‚™è¨“ç·´æ•¸æ“šé›†...")
        
        def tokenize_function(examples):
            """å°æ–‡æœ¬é€²è¡Œ tokenization"""
            texts = [text + self.tokenizer.eos_token for text in examples['text']]
            
            model_inputs = self.tokenizer(
                texts,
                truncation=True,
                padding=False,
                max_length=self.config.get('max_length', 512),
                return_tensors=None
            )
            
            model_inputs["labels"] = model_inputs["input_ids"].copy()
            return model_inputs
            
        self.tokenized_dataset = self.dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=self.dataset.column_names,
            desc="Tokenizing æ•¸æ“šé›†"
        )
        
        print(f"Tokenized æ•¸æ“šé›†å¤§å°: {len(self.tokenized_dataset)}")
        
    def setup_training(self):
        """è¨­ç½®è¨“ç·´åƒæ•¸å’Œ trainer"""
        print("\nâš™ï¸  è¨­ç½®è¨“ç·´åƒæ•¸...")
        
        training_args = TrainingArguments(
            output_dir=self.config.get('output_dir', "./results"),
            num_train_epochs=self.config.get('num_epochs', 1),
            per_device_train_batch_size=self.config.get('batch_size', 4),
            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 4),
            optim="paged_adamw_32bit",
            save_steps=self.config.get('save_steps', 100),
            logging_steps=self.config.get('logging_steps', 25),
            learning_rate=self.config.get('learning_rate', 2e-5),
            weight_decay=self.config.get('weight_decay', 0.001),
            bf16=True,
            max_grad_norm=self.config.get('max_grad_norm', 0.3),
            max_steps=-1,
            warmup_ratio=self.config.get('warmup_ratio', 0.03),
            group_by_length=True,
            lr_scheduler_type="constant",
            report_to="tensorboard",
            save_total_limit=2,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
        )
        
        # è¨­ç½®æ•¸æ“šæ”¶é›†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )
        
        # åˆå§‹åŒ– Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.tokenized_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        print("âœ… è¨“ç·´è¨­ç½®å®Œæˆ!")
        print(f"æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
        print(f"æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {training_args.gradient_accumulation_steps}")
        print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
        print(f"å­¸ç¿’ç‡: {training_args.learning_rate}")
        print(f"è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}")
        
    def train(self):
        """é–‹å§‹è¨“ç·´"""
        print("\nğŸš€ é–‹å§‹è¨“ç·´...")
        
        try:
            self.trainer.train()
            print("âœ… è¨“ç·´æˆåŠŸå®Œæˆ!")
        except Exception as e:
            print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
            print("å¸¸è¦‹è§£æ±ºæ–¹æ¡ˆ:")
            print("1. æ¸›å°‘ batch_size æˆ– max_length")
            print("2. æª¢æŸ¥æ¨¡å‹å­˜å–æ¬Šé™")
            print("3. æª¢æŸ¥ CUDA ç›¸å®¹æ€§")
            raise
            
    def save_model(self):
        """ä¿å­˜å¾®èª¿æ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜å¾®èª¿æ¨¡å‹...")
        
        output_dir = self.config.get('model_output_dir', "./finetuned_medgemma_4b")
        
        # ä¿å­˜æ¨¡å‹å’Œ tokenizer
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜è¨“ç·´é…ç½®
        training_config = {
            "model_id": self.config['model_id'],
            "trainer_type": "normal_trainer",
            "lora_config": {
                "r": self.config.get('lora_r', 16),
                "lora_alpha": self.config.get('lora_alpha', 32),
                "lora_dropout": self.config.get('lora_dropout', 0.05),
                "target_modules": self.config.get('target_modules', ["q_proj", "v_proj"]),
            },
            "training_args": {
                "learning_rate": self.config.get('learning_rate', 2e-5),
                "num_train_epochs": self.config.get('num_epochs', 1),
                "per_device_train_batch_size": self.config.get('batch_size', 4),
                "gradient_accumulation_steps": self.config.get('gradient_accumulation_steps', 4),
            },
            "dataset_size": len(self.tokenized_dataset),
        }
        
        with open(os.path.join(output_dir, "training_config.json"), "w", encoding='utf-8') as f:
            json.dump(training_config, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
    def test_model(self):
        """æ¸¬è©¦å¾®èª¿æ¨¡å‹"""
        print("\nğŸ§ª æ¸¬è©¦å¾®èª¿æ¨¡å‹...")
        
        # æ¸…ç† GPU è¨˜æ†¶é«”
        torch.cuda.empty_cache()
        
        # é‡æ–°è¼‰å…¥æ¨¡å‹é€²è¡Œæ¨ç†
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            self.config['model_id'],
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        
        output_dir = self.config.get('model_output_dir', "./finetuned_medgemma_4b")
        finetuned_model = PeftModel.from_pretrained(base_model, output_dir)
        finetuned_model.eval()
        
        # æ¸¬è©¦å•é¡Œ
        test_questions = [
            "ç³–å°¿ç—…çš„ç—‡ç‹€æ˜¯ä»€éº¼?",
            "é«˜è¡€å£“å¦‚ä½•æ²»ç™‚?",
            "å¿ƒè‡Ÿç—…çš„åŸå› æ˜¯ä»€éº¼?",
        ]
        
        print("æ¸¬è©¦è‡ªå®šç¾©å•é¡Œ:")
        print("=" * 60)
        
        for i, question in enumerate(test_questions, 1):
            print(f"\næ¸¬è©¦ {i}:")
            print(f"å•é¡Œ: {question}")
            
            test_input = f"Instruction: {question}\nResponse:"
            inputs = self.tokenizer(test_input, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = finetuned_model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "Response:" in response:
                generated_response = response.split("Response:")[-1].strip()
            else:
                generated_response = response
                
            print(f"å›æ‡‰: {generated_response}")
            print("-" * 40)
            
    def run(self):
        """é‹è¡Œå®Œæ•´çš„è¨“ç·´æµç¨‹"""
        print("ğŸ¯ é–‹å§‹ MedGemma 27B ä¸­æ–‡å¾®èª¿æµç¨‹")
        print("=" * 60)
        
        # æª¢æŸ¥ GPU
        self.check_gpu()
        
        # èªè­‰
        if not self.authenticate_huggingface():
            return False
            
        # è¼‰å…¥æ•¸æ“šé›†
        self.load_dataset()
        
        # è¨­ç½®æ¨¡å‹
        self.setup_model()
        
        # è¨­ç½® LoRA
        self.setup_lora()
        
        # æº–å‚™æ•¸æ“šé›†
        self.prepare_dataset()
        
        # è¨­ç½®è¨“ç·´
        self.setup_training()
        
        # é–‹å§‹è¨“ç·´
        self.train()
        
        # ä¿å­˜æ¨¡å‹
        self.save_model()
        
        # æ¸¬è©¦æ¨¡å‹
        self.test_model()
        
        print("\nğŸ‰ è¨“ç·´æµç¨‹å®Œæˆ!")
        print("=" * 60)
        return True

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="MedGemma 27B ä¸­æ–‡å¾®èª¿è¨“ç·´è…³æœ¬")
    parser.add_argument("--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--data_path", type=str, default="medquad.csv", help="æ•¸æ“šé›†è·¯å¾‘")
    parser.add_argument("--output_dir", type=str, default="./results", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--model_output_dir", type=str, default="./finetuned_medgemma_4b", help="æ¨¡å‹è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--sample_size", type=int, default=2000, help="æ•¸æ“šé›†æ¨£æœ¬å¤§å°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•·åº¦")
    parser.add_argument("--num_epochs", type=int, default=1, help="è¨“ç·´è¼ªæ•¸")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="å­¸ç¿’ç‡")
    
    args = parser.parse_args()
    
    # é»˜èªé…ç½®
    config = {
        'model_id': 'google/medgemma-4b-multimodal',
        'data_path': args.data_path,
        'output_dir': args.output_dir,
        'model_output_dir': args.model_output_dir,
        'sample_size': args.sample_size,
        'batch_size': args.batch_size,
        'max_length': args.max_length,
        'num_epochs': args.num_epochs,
        'learning_rate': args.learning_rate,
        'lora_r': 16,
        'lora_alpha': 32,
        'lora_dropout': 0.05,
        'target_modules': ["q_proj", "v_proj"],
        'gradient_accumulation_steps': 4,
        'save_steps': 100,
        'logging_steps': 25,
        'weight_decay': 0.001,
        'max_grad_norm': 0.3,
        'warmup_ratio': 0.03,
    }
    
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œå‰‡è¼‰å…¥
    if os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            file_config = json.load(f)
            config.update(file_config)
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(config['output_dir'], exist_ok=True)
    os.makedirs(config['model_output_dir'], exist_ok=True)
    
    # é‹è¡Œè¨“ç·´
    trainer = MedGemmaTrainer(config)
    success = trainer.run()
    
    if success:
        print("\nâœ… è¨“ç·´æˆåŠŸå®Œæˆ!")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {config['model_output_dir']}")
        print("æ‚¨å¯ä»¥ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹é€²è¡Œé†«ç™‚å•ç­”!")
    else:
        print("\nâŒ è¨“ç·´å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main() 