# ğŸš€ MedGemma 27B ä¸­æ–‡å¾®èª¿ - å¿«é€Ÿé–‹å§‹æŒ‡å—

## ğŸ“‹ å‰ç½®éœ€æ±‚

### ç¡¬é«”éœ€æ±‚
- **GPU**: NVIDIA GPUï¼Œè‡³å°‘ 24GB VRAM (æ¨è–¦ RTX 4090)
- **è¨˜æ†¶é«”**: è‡³å°‘ 32GB ç³»çµ±è¨˜æ†¶é«”
- **å„²å­˜**: è‡³å°‘ 50GB å¯ç”¨ç©ºé–“

### è»Ÿé«”éœ€æ±‚
- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬

## âš¡ å¿«é€Ÿå®‰è£

### 1. å…‹éš†å°ˆæ¡ˆ
```bash
git clone https://github.com/your-username/Fine-tune-MedGemma.git
cd Fine-tune-MedGemma
```

### 2. å‰µå»ºè™›æ“¬ç’°å¢ƒ
```bash
# ä½¿ç”¨ conda
conda create -n medgemma python=3.10
conda activate medgemma

# æˆ–ä½¿ç”¨ venv
python -m venv medgemma_env
source medgemma_env/bin/activate  # Linux/Mac
# æˆ–
medgemma_env\Scripts\activate  # Windows
```

### 3. å®‰è£ä¾è³´
```bash
pip install -r requirements.txt
```

### 4. å®‰è£ PyTorch (æ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬)
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ” æ¨¡å‹å­˜å–è¨­ç½®

### 1. ç”³è«‹ MedGemma å­˜å–æ¬Šé™
1. å‰å¾€ [MedGemma æ¨¡å‹é é¢](https://huggingface.co/google/medgemma-27b-multimodal)
2. é»æ“Š "Request Access"
3. æ¥å—æˆæ¬Šå”è­°
4. ç­‰å¾…å¯©æ ¸é€šé

### 2. å–å¾— HuggingFace Token
1. å‰å¾€ [Token è¨­ç½®é é¢](https://huggingface.co/settings/tokens)
2. é»æ“Š "New token"
3. é¸æ“‡ "Read" æ¬Šé™
4. è¤‡è£½ç”Ÿæˆçš„ token

### 3. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```bash
# Linux/Mac
export HUGGINGFACE_HUB_TOKEN="your_token_here"

# Windows
set HUGGINGFACE_HUB_TOKEN=your_token_here
```

## ğŸ“Š æ•¸æ“šæº–å‚™

### 1. æº–å‚™ MedQuAD æ•¸æ“šé›†
ç¢ºä¿ `medquad.csv` æ–‡ä»¶åœ¨å°ˆæ¡ˆç›®éŒ„ä¸­ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- `question`: é†«ç™‚å•é¡Œ
- `answer`: é†«ç™‚å›ç­”
- `source`: æ•¸æ“šä¾†æº
- `focus_area`: å°ˆæ³¨é ˜åŸŸ

### 2. æ•¸æ“šæ ¼å¼ç¯„ä¾‹
```csv
question,answer,source,focus_area
"ä»€éº¼æ˜¯ç³–å°¿ç—…?","ç³–å°¿ç—…æ˜¯ä¸€ç¨®ä»£è¬ç–¾ç—…...",medical_textbook,endocrinology
"é«˜è¡€å£“çš„ç—‡ç‹€æœ‰å“ªäº›?","é«˜è¡€å£“çš„ç—‡ç‹€åŒ…æ‹¬...",medical_textbook,cardiology
```

## ğŸš€ é–‹å§‹è¨“ç·´

### æ–¹æ³• 1: ä½¿ç”¨ Jupyter Notebook
```bash
jupyter notebook finetune_medgemma_27b_ä¸­æ–‡.ipynb
```

### æ–¹æ³• 2: ä½¿ç”¨ Python è…³æœ¬
```bash
# ä½¿ç”¨é»˜èªé…ç½®
python train_medgemma_27b.py

# ä½¿ç”¨è‡ªå®šç¾©é…ç½®
python train_medgemma_27b.py --config config.json

# èª¿æ•´åƒæ•¸
python train_medgemma_27b.py --batch_size 2 --sample_size 1000 --num_epochs 2
```

### æ–¹æ³• 3: ä½¿ç”¨é…ç½®æ–‡ä»¶
```bash
# ç·¨è¼¯ config.json èª¿æ•´åƒæ•¸
python train_medgemma_27b.py --config config.json
```

## ğŸ“ˆ ç›£æ§è¨“ç·´

### 1. GPU è¨˜æ†¶é«”ç›£æ§
```bash
nvidia-smi -l 1
```

### 2. TensorBoard ç›£æ§
```bash
tensorboard --logdir ./results
```

### 3. è¨“ç·´æ—¥èªŒ
è¨“ç·´æ—¥èªŒä¿å­˜åœ¨ `./results/` ç›®éŒ„ä¸­

## ğŸ§ª æ¸¬è©¦æ¨¡å‹

### 1. è¼‰å…¥å¾®èª¿æ¨¡å‹
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# è¼‰å…¥åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("google/medgemma-27b-multimodal")
tokenizer = AutoTokenizer.from_pretrained("google/medgemma-27b-multimodal")

# è¼‰å…¥ LoRA é©é…å™¨
finetuned_model = PeftModel.from_pretrained(base_model, "./finetuned_medgemma_27b")
```

### 2. ç”Ÿæˆé†«ç™‚å›ç­”
```python
def generate_medical_response(question):
    test_input = f"Instruction: {question}\nResponse:"
    inputs = tokenizer(test_input, return_tensors="pt")
    
    with torch.no_grad():
        outputs = finetuned_model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Response:")[-1].strip()

# æ¸¬è©¦
question = "ç³–å°¿ç—…çš„ç—‡ç‹€æ˜¯ä»€éº¼?"
answer = generate_medical_response(question)
print(f"å•é¡Œ: {question}")
print(f"å›ç­”: {answer}")
```

## ğŸ”§ å¸¸è¦‹å•é¡Œ

### 1. CUDA è¨˜æ†¶é«”ä¸è¶³
**è§£æ±ºæ–¹æ¡ˆ**:
- æ¸›å°‘ `batch_size` (å¾ 4 æ”¹ç‚º 2)
- æ¸›å°‘ `max_length` (å¾ 512 æ”¹ç‚º 256)
- å¢åŠ  `gradient_accumulation_steps` (å¾ 4 æ”¹ç‚º 8)

### 2. æ¨¡å‹å­˜å–è¢«æ‹’çµ•
**è§£æ±ºæ–¹æ¡ˆ**:
- ç¢ºèªå·²ç”³è«‹ MedGemma æ¨¡å‹å­˜å–æ¬Šé™
- æª¢æŸ¥ HuggingFace token æ˜¯å¦æ­£ç¢º
- é‡æ–°åŸ·è¡Œèªè­‰

### 3. å¥—ä»¶ç‰ˆæœ¬è¡çª
**è§£æ±ºæ–¹æ¡ˆ**:
- ä½¿ç”¨è™›æ“¬ç’°å¢ƒ
- æ›´æ–°åˆ°æŒ‡å®šç‰ˆæœ¬: `pip install transformers==4.44.0`
- é‡æ–°å®‰è£è¡çªå¥—ä»¶

## ğŸ“Š æ€§èƒ½åŸºæº–

| GPU å‹è™Ÿ | VRAM | æ‰¹æ¬¡å¤§å° | åºåˆ—é•·åº¦ | è¨“ç·´æ™‚é–“ |
|----------|------|----------|----------|----------|
| RTX 4090 | 24GB | 4 | 512 | ~2å°æ™‚ |
| RTX 3090 | 24GB | 2 | 512 | ~4å°æ™‚ |
| RTX 3080 | 10GB | 1 | 256 | ~8å°æ™‚ |

## âš ï¸ é‡è¦æé†’

- **é†«ç™‚å…è²¬è²æ˜**: æ­¤æ¨¡å‹åƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”
- **æ•¸æ“šéš±ç§**: ç¢ºä¿ä½¿ç”¨é©ç•¶çš„é†«ç™‚æ•¸æ“š
- **æ¨¡å‹é™åˆ¶**: å¯èƒ½å­˜åœ¨åè¦‹å’Œä¸æº–ç¢ºæ€§

## ğŸ“ æ”¯æ´

å¦‚æœé‡åˆ°å•é¡Œï¼š
1. æŸ¥çœ‹ `README_ä¸­æ–‡.md` çš„è©³ç´°èªªæ˜
2. æª¢æŸ¥æ•…éšœæ’é™¤éƒ¨åˆ†
3. å‰µå»º Issue ä¸¦æä¾›è©³ç´°è³‡è¨Š

---

**ç¥æ‚¨è¨“ç·´é †åˆ©ï¼** ğŸš€ 