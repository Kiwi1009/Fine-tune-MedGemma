# MedGemma 27B ä¸­æ–‡å¾®èª¿å°ˆæ¡ˆ

<div align="center">

![MedGemma Logo](https://img.shields.io/badge/MedGemma-27B-blue?style=for-the-badge&logo=google)
![Python](https://img.shields.io/badge/Python-3.8+-green?style=for-the-badge&logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?style=for-the-badge&logo=pytorch)
![GPU](https://img.shields.io/badge/GPU-RTX%204090-orange?style=for-the-badge&logo=nvidia)

**åŸºæ–¼ MedGemma 27B å¤šæ¨¡æ…‹æ¨¡å‹çš„é†«ç™‚å•ç­”å¾®èª¿å°ˆæ¡ˆ**

[å¿«é€Ÿé–‹å§‹](#-å¿«é€Ÿé–‹å§‹) â€¢ [å°ˆæ¡ˆæ¶æ§‹](#-å°ˆæ¡ˆæ¶æ§‹) â€¢ [ç¨‹å¼ç¢¼èªªæ˜](#-ç¨‹å¼ç¢¼èªªæ˜) â€¢ [ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—) â€¢ [æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)

</div>

---

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ˜¯ä¸€å€‹å®Œæ•´çš„ MedGemma 27B ä¸­æ–‡å¾®èª¿è§£æ±ºæ–¹æ¡ˆï¼Œå°ˆé–€é‡å°é†«ç™‚å•ç­”ä»»å‹™é€²è¡Œå„ªåŒ–ã€‚å°ˆæ¡ˆæ¡ç”¨ LoRA (Low-Rank Adaptation) æŠ€è¡“ï¼Œåœ¨ä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ™‚ï¼Œé«˜æ•ˆåœ°é©æ‡‰é†«ç™‚é ˜åŸŸçš„ç‰¹å®šéœ€æ±‚ã€‚

### ğŸ¯ ä¸»è¦ç‰¹è‰²

- **ğŸ–¥ï¸ GPU å„ªåŒ–**: é‡å° RTX 4090 ç­‰é«˜ç«¯é¡¯å¡é€²è¡Œè¨˜æ†¶é«”å„ªåŒ–
- **ğŸ”§ LoRA å¾®èª¿**: ä½¿ç”¨ä½ç§©é©æ‡‰æŠ€è¡“ï¼Œé«˜æ•ˆæ›´æ–°æ¨¡å‹åƒæ•¸
- **ğŸ¥ é†«ç™‚å°ˆç”¨**: åŸºæ–¼ MedQuAD é†«ç™‚å•ç­”æ•¸æ“šé›†è¨“ç·´
- **ğŸ“Š è¨˜æ†¶é«”ç›£æ§**: å…§å»º GPU è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤å’Œå„ªåŒ–å»ºè­°
- **ğŸŒ ä¸­æ–‡æ”¯æ´**: å®Œæ•´çš„ä¸­æ–‡èªªæ˜å’ŒéŒ¯èª¤è™•ç†
- **ğŸ“ˆ æ€§èƒ½åŸºæº–**: ä¸åŒç¡¬é«”é…ç½®çš„è¨“ç·´æ™‚é–“é ä¼°

### âš¡ æŠ€è¡“äº®é»

- **4-bit é‡åŒ–**: å¤§å¹…æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ï¼Œæ”¯æ´æ›´å¤§æ¨¡å‹
- **æ··åˆç²¾åº¦è¨“ç·´**: ä½¿ç”¨ BF16 ç²¾åº¦åŠ é€Ÿè¨“ç·´éç¨‹
- **æ¢¯åº¦æª¢æŸ¥é»**: å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨ï¼Œæ”¯æ´æ›´é•·åºåˆ—
- **å‹•æ…‹æ‰¹æ¬¡è™•ç†**: æ™ºèƒ½èª¿æ•´æ‰¹æ¬¡å¤§å°ä»¥é©æ‡‰è¨˜æ†¶é«”é™åˆ¶

---

## ğŸ—ï¸ å°ˆæ¡ˆæ¶æ§‹

```
Fine-tune-MedGemma/
â”œâ”€â”€ ğŸ“ æ ¸å¿ƒè¨“ç·´æ–‡ä»¶
â”‚   â”œâ”€â”€ finetune_medgemma_27b_ä¸­æ–‡.ipynb    # Jupyter Notebook ç‰ˆæœ¬
â”‚   â”œâ”€â”€ train_medgemma_27b.py              # Python è…³æœ¬ç‰ˆæœ¬
â”‚   â””â”€â”€ config.json                        # è¨“ç·´é…ç½®æ–‡ä»¶
â”‚
â”œâ”€â”€ ğŸ“ æ–‡æª”èªªæ˜
â”‚   â”œâ”€â”€ README_ä¸­æ–‡.md                      # è©³ç´°ä¸­æ–‡èªªæ˜
â”‚   â”œâ”€â”€ å¿«é€Ÿé–‹å§‹æŒ‡å—.md                     # ç°¡æ½”å¿«é€ŸæŒ‡å—
â”‚   â””â”€â”€ upload_to_github.md                # GitHub ä¸Šå‚³æŒ‡å—
â”‚
â”œâ”€â”€ ğŸ“ ä¾è³´é…ç½®
â”‚   â”œâ”€â”€ requirements.txt                    # Python ä¾è³´å¥—ä»¶
â”‚   â””â”€â”€ medquad.csv                        # é†«ç™‚å•ç­”æ•¸æ“šé›†
â”‚
â””â”€â”€ ğŸ“ è¼¸å‡ºç›®éŒ„ (è¨“ç·´æ™‚ç”Ÿæˆ)
    â”œâ”€â”€ ./results/                         # è¨“ç·´çµæœå’Œæ—¥èªŒ
    â””â”€â”€ ./finetuned_medgemma_4b/          # å¾®èª¿æ¨¡å‹è¼¸å‡º
```

### ğŸ”§ æ ¸å¿ƒçµ„ä»¶èªªæ˜

#### 1. è¨“ç·´å¼•æ“ (`train_medgemma_27b.py`)
```python
class MedGemmaTrainer:
    """MedGemma è¨“ç·´å™¨é¡ - æ ¸å¿ƒè¨“ç·´é‚è¼¯"""
    
    def __init__(self, config):
        # åˆå§‹åŒ–è¨“ç·´å™¨
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def check_gpu(self):
        # GPU å¯ç”¨æ€§æª¢æŸ¥å’Œè¨˜æ†¶é«”è©•ä¼°
        
    def authenticate_huggingface(self):
        # HuggingFace èªè­‰å’Œæ¨¡å‹å­˜å–é©—è­‰
        
    def load_dataset(self):
        # æ•¸æ“šé›†è¼‰å…¥å’Œé è™•ç†
        
    def setup_model(self):
        # æ¨¡å‹è¼‰å…¥å’Œé‡åŒ–é…ç½®
        
    def setup_lora(self):
        # LoRA é…ç½®å’Œé©é…å™¨è¨­ç½®
        
    def prepare_dataset(self):
        # æ•¸æ“šé›† tokenization å’Œæ ¼å¼åŒ–
        
    def setup_training(self):
        # è¨“ç·´åƒæ•¸é…ç½®å’Œ Trainer åˆå§‹åŒ–
        
    def train(self):
        # åŸ·è¡Œè¨“ç·´éç¨‹
        
    def save_model(self):
        # ä¿å­˜å¾®èª¿æ¨¡å‹å’Œé…ç½®
        
    def test_model(self):
        # æ¨¡å‹æ¸¬è©¦å’Œæ¨ç†é©—è­‰
```

#### 2. é…ç½®ç®¡ç† (`config.json`)
```json
{
  "model_id": "google/medgemma-4b-multimodal",
  "data_path": "medquad.csv",
  "output_dir": "./results",
  "model_output_dir": "./finetuned_medgemma_4b",
  "sample_size": 2000,
  "batch_size": 4,
  "max_length": 512,
  "num_epochs": 1,
  "learning_rate": 2e-5,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "target_modules": ["q_proj", "v_proj"],
  "gradient_accumulation_steps": 4
}
```

#### 3. æ•¸æ“šè™•ç†æµç¨‹
```python
# æ•¸æ“šé è™•ç†æµç¨‹
def format_instruction(row):
    """å°‡é†«ç™‚å•ç­”æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤-å›æ‡‰æ ¼å¼"""
    return f"Instruction: {row['question']}\nResponse: {row['answer']}"

# Tokenization æµç¨‹
def tokenize_function(examples):
    """å°æ–‡æœ¬é€²è¡Œ tokenizationï¼Œé©ç”¨æ–¼å› æœèªè¨€å»ºæ¨¡"""
    texts = [text + tokenizer.eos_token for text in examples['text']]
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding=False,
        max_length=512,
        return_tensors=None
    )
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs
```

---

## ğŸ’» ç¨‹å¼ç¢¼èªªæ˜

### ğŸ”§ æ ¸å¿ƒæŠ€è¡“å¯¦ç¾

#### 1. æ¨¡å‹é‡åŒ–é…ç½®
```python
# 4-bit é‡åŒ–é…ç½®ï¼Œå¤§å¹…æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # å•Ÿç”¨ 4-bit é‡åŒ–
    bnb_4bit_use_double_quant=True,       # é›™é‡é‡åŒ–
    bnb_4bit_quant_type="nf4",           # ä½¿ç”¨ NF4 é‡åŒ–é¡å‹
    bnb_4bit_compute_dtype=torch.bfloat16 # è¨ˆç®—ç²¾åº¦
)
```

#### 2. LoRA é©é…å™¨é…ç½®
```python
# LoRA é…ç½®ï¼Œé«˜æ•ˆåƒæ•¸é©æ‡‰
lora_config = LoraConfig(
    r=16,                    # LoRA ç§©ï¼Œæ§åˆ¶é©é…å™¨å¤§å°
    lora_alpha=32,           # ç¸®æ”¾åƒæ•¸ï¼Œå½±éŸ¿å­¸ç¿’ç‡
    lora_dropout=0.05,       # Dropout ç‡ï¼Œé˜²æ­¢éæ“¬åˆ
    bias="none",             # ä¸æ›´æ–°åç½®é …
    task_type=TaskType.CAUSAL_LM,  # å› æœèªè¨€å»ºæ¨¡ä»»å‹™
    target_modules=["q_proj", "v_proj"],  # ç›®æ¨™æ³¨æ„åŠ›æ¨¡çµ„
)
```

#### 3. è¨“ç·´åƒæ•¸å„ªåŒ–
```python
# é‡å° GPU è¨˜æ†¶é«”å„ªåŒ–çš„è¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir="./results",                    # è¼¸å‡ºç›®éŒ„
    num_train_epochs=1,                       # è¨“ç·´è¼ªæ•¸
    per_device_train_batch_size=4,            # æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,            # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
    learning_rate=2e-5,                       # å­¸ç¿’ç‡
    weight_decay=0.001,                       # æ¬Šé‡è¡°æ¸›
    bf16=True,                                # æ··åˆç²¾åº¦è¨“ç·´
    max_grad_norm=0.3,                        # æ¢¯åº¦è£å‰ª
    warmup_ratio=0.03,                        # ç†±èº«æ¯”ä¾‹
    group_by_length=True,                      # æŒ‰é•·åº¦åˆ†çµ„
    lr_scheduler_type="constant",             # å­¸ç¿’ç‡èª¿åº¦å™¨
    report_to="tensorboard",                  # å ±å‘Šåˆ° TensorBoard
)
```

#### 4. è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥
```python
# GPU è¨˜æ†¶é«”ç›£æ§å’Œå„ªåŒ–
def check_gpu(self):
    """æª¢æŸ¥ GPU å¯ç”¨æ€§å’Œè¨˜æ†¶é«”"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory < 20:
            print("âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”ä¸è¶³ 20GBï¼Œå»ºè­°èª¿æ•´æ‰¹æ¬¡å¤§å°")
        elif gpu_memory >= 24:
            print("âœ… GPU è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨é è¨­è¨­ç½®")

# æ¢¯åº¦æª¢æŸ¥é»å•Ÿç”¨
model.gradient_checkpointing_enable()

# å‹•æ…‹è¨˜æ†¶é«”æ¸…ç†
torch.cuda.empty_cache()
```

### ğŸ“Š æ•¸æ“šè™•ç†æ¶æ§‹

#### 1. æ•¸æ“šè¼‰å…¥å’Œé è™•ç†
```python
def load_dataset(self):
    """è¼‰å…¥å’Œé è™•ç† MedQuAD æ•¸æ“šé›†"""
    df = pd.read_csv(self.config['data_path'])
    
    # æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤-å›æ‡‰æ ¼å¼
    def format_instruction(row):
        return f"Instruction: {row['question']}\nResponse: {row['answer']}"
    
    df['text'] = df.apply(format_instruction, axis=1)
    
    # å–æ¨£æ§åˆ¶è¨“ç·´æ™‚é–“
    sample_size = min(self.config.get('sample_size', 2000), len(df))
    df_sample = df.sample(n=sample_size, random_state=42)
    
    # è½‰æ›ç‚º HuggingFace Dataset
    self.dataset = Dataset.from_pandas(df_sample[['text']])
```

#### 2. Tokenization å’Œæ‰¹æ¬¡è™•ç†
```python
def prepare_dataset(self):
    """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
    def tokenize_function(examples):
        texts = [text + self.tokenizer.eos_token for text in examples['text']]
        
        model_inputs = self.tokenizer(
            texts,
            truncation=True,
            padding=False,
            max_length=self.config.get('max_length', 512),
            return_tensors=None
        )
        
        # è¨­ç½®æ¨™ç±¤é€²è¡Œå› æœèªè¨€å»ºæ¨¡
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs
    
    self.tokenized_dataset = self.dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=self.dataset.column_names,
        desc="Tokenizing æ•¸æ“šé›†"
    )
```

### ğŸ§ª æ¨¡å‹æ¸¬è©¦å’Œæ¨ç†

#### 1. æ¨¡å‹è¼‰å…¥å’Œæ¨ç†
```python
def test_model(self):
    """æ¸¬è©¦å¾®èª¿æ¨¡å‹"""
    # é‡æ–°è¼‰å…¥æ¨¡å‹é€²è¡Œæ¨ç†
    base_model = AutoModelForCausalLM.from_pretrained(
        self.config['model_id'],
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    
    # è¼‰å…¥ LoRA é©é…å™¨
    finetuned_model = PeftModel.from_pretrained(base_model, output_dir)
    finetuned_model.eval()
    
    # æ¸¬è©¦æ¨ç†
    test_questions = [
        "ç³–å°¿ç—…çš„ç—‡ç‹€æ˜¯ä»€éº¼?",
        "é«˜è¡€å£“å¦‚ä½•æ²»ç™‚?",
        "å¿ƒè‡Ÿç—…çš„åŸå› æ˜¯ä»€éº¼?",
    ]
    
    for question in test_questions:
        test_input = f"Instruction: {question}\nResponse:"
        inputs = self.tokenizer(test_input, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = finetuned_model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_response = response.split("Response:")[-1].strip()
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ğŸ“‹ ç³»çµ±éœ€æ±‚

#### ç¡¬é«”éœ€æ±‚
- **GPU**: NVIDIA GPUï¼Œè‡³å°‘ 24GB VRAM (æ¨è–¦ RTX 4090 æˆ– A100)
- **è¨˜æ†¶é«”**: è‡³å°‘ 32GB ç³»çµ±è¨˜æ†¶é«”
- **å„²å­˜**: è‡³å°‘ 50GB å¯ç”¨ç©ºé–“

#### è»Ÿé«”éœ€æ±‚
- **ä½œæ¥­ç³»çµ±**: Windows 10/11, Linux, æˆ– macOS
- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬ (èˆ‡ PyTorch ç›¸å®¹)

### âš¡ å¿«é€Ÿå®‰è£

#### 1. ç’°å¢ƒæº–å‚™
```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create -n medgemma python=3.10
conda activate medgemma

# æˆ–ä½¿ç”¨ venv
python -m venv medgemma_env
source medgemma_env/bin/activate  # Linux/Mac
medgemma_env\Scripts\activate     # Windows
```

#### 2. å®‰è£ä¾è³´
```bash
# å®‰è£ PyTorch (æ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt
```

#### 3. é©—è­‰å®‰è£
```python
import torch
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU å‹è™Ÿ: {torch.cuda.get_device_name(0)}")
    print(f"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

### ğŸ” æ¨¡å‹å­˜å–è¨­ç½®

#### 1. ç”³è«‹ MedGemma å­˜å–æ¬Šé™
1. å‰å¾€ [MedGemma æ¨¡å‹é é¢](https://huggingface.co/google/medgemma-4b-multimodal)
2. é»æ“Š "Request Access" ç”³è«‹å­˜å–æ¬Šé™
3. æ¥å—æˆæ¬Šå”è­°
4. ç­‰å¾…å¯©æ ¸é€šé (é€šå¸¸éœ€è¦å¹¾åˆ†é˜åˆ°å¹¾å°æ™‚)

#### 2. å–å¾— HuggingFace Token
1. å‰å¾€ [Token è¨­ç½®é é¢](https://huggingface.co/settings/tokens)
2. é»æ“Š "New token"
3. é¸æ“‡ "Read" æ¬Šé™
4. è¤‡è£½ç”Ÿæˆçš„ token

#### 3. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```bash
# Linux/Mac
export HUGGINGFACE_HUB_TOKEN="your_token_here"

# Windows
set HUGGINGFACE_HUB_TOKEN=your_token_here
```

### ğŸš€ é–‹å§‹è¨“ç·´

#### æ–¹æ³• 1: ä½¿ç”¨ Jupyter Notebook
```bash
jupyter notebook finetune_medgemma_27b_ä¸­æ–‡.ipynb
```

#### æ–¹æ³• 2: ä½¿ç”¨ Python è…³æœ¬
```bash
# ä½¿ç”¨é»˜èªé…ç½®
python train_medgemma_27b.py

# ä½¿ç”¨è‡ªå®šç¾©é…ç½®
python train_medgemma_27b.py --config config.json

# èª¿æ•´åƒæ•¸
python train_medgemma_27b.py --batch_size 2 --sample_size 1000 --num_epochs 2
```

---

## ğŸ“Š ä½¿ç”¨æŒ‡å—

### ğŸ›ï¸ é…ç½®åƒæ•¸èªªæ˜

#### æ¨¡å‹é…ç½®
| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | å»ºè­°ç¯„åœ |
|------|------|--------|----------|
| `model_id` | åŸºç¤æ¨¡å‹ ID | `google/medgemma-4b-multimodal` | - |
| `sample_size` | è¨“ç·´æ¨£æœ¬æ•¸é‡ | 2000 | 1000-5000 |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 4 | 1-8 |
| `max_length` | æœ€å¤§åºåˆ—é•·åº¦ | 512 | 256-1024 |

#### LoRA é…ç½®
| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | å»ºè­°ç¯„åœ |
|------|------|--------|----------|
| `lora_r` | LoRA ç§© | 16 | 8-32 |
| `lora_alpha` | ç¸®æ”¾åƒæ•¸ | 32 | 16-64 |
| `lora_dropout` | Dropout ç‡ | 0.05 | 0.01-0.1 |

#### è¨“ç·´é…ç½®
| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | å»ºè­°ç¯„åœ |
|------|------|--------|----------|
| `learning_rate` | å­¸ç¿’ç‡ | 2e-5 | 1e-5 - 5e-5 |
| `num_epochs` | è¨“ç·´è¼ªæ•¸ | 1 | 1-5 |
| `gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç©æ­¥æ•¸ | 4 | 2-8 |

### ğŸ“ˆ æ€§èƒ½åŸºæº–

#### ç¡¬é«”é…ç½®å»ºè­°
| GPU å‹è™Ÿ | VRAM | æ‰¹æ¬¡å¤§å° | åºåˆ—é•·åº¦ | è¨“ç·´æ™‚é–“ | è¨˜æ†¶é«”ä½¿ç”¨ |
|----------|------|----------|----------|----------|------------|
| RTX 4090 | 24GB | 4 | 512 | ~2å°æ™‚ | 18-22GB |
| RTX 3090 | 24GB | 2 | 512 | ~4å°æ™‚ | 16-20GB |
| RTX 3080 | 10GB | 1 | 256 | ~8å°æ™‚ | 8-12GB |

#### è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ
- **æ¨¡å‹è¼‰å…¥**: ~8GB (4-bit é‡åŒ–)
- **è¨“ç·´éç¨‹**: ~18-22GB (åŒ…å«æ¢¯åº¦ã€å„ªåŒ–å™¨ç‹€æ…‹)
- **ç·©è¡å€**: ~2-4GB (æ•¸æ“šè¼‰å…¥å’Œç·©å­˜)

### ğŸ§ª æ¨¡å‹æ¸¬è©¦

#### 1. è¼‰å…¥å¾®èª¿æ¨¡å‹
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# è¼‰å…¥åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("google/medgemma-4b-multimodal")
tokenizer = AutoTokenizer.from_pretrained("google/medgemma-4b-multimodal")

# è¼‰å…¥ LoRA é©é…å™¨
finetuned_model = PeftModel.from_pretrained(base_model, "./finetuned_medgemma_4b")
```

#### 2. ç”Ÿæˆé†«ç™‚å›ç­”
```python
def generate_medical_response(question, max_length=256):
    test_input = f"Instruction: {question}\nResponse:"
    inputs = tokenizer(test_input, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = finetuned_model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Response:")[-1].strip()

# æ¸¬è©¦
questions = [
    "ç³–å°¿ç—…çš„ç—‡ç‹€æ˜¯ä»€éº¼?",
    "é«˜è¡€å£“å¦‚ä½•æ²»ç™‚?",
    "å¿ƒè‡Ÿç—…çš„åŸå› æ˜¯ä»€éº¼?",
]

for question in questions:
    answer = generate_medical_response(question)
    print(f"å•é¡Œ: {question}")
    print(f"å›ç­”: {answer}\n")
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹éŒ¯èª¤åŠè§£æ±ºæ–¹æ¡ˆ

#### 1. CUDA è¨˜æ†¶é«”ä¸è¶³
```
RuntimeError: CUDA out of memory
```

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size=2

# æ¸›å°‘åºåˆ—é•·åº¦
max_length=256

# å¢åŠ æ¢¯åº¦ç´¯ç©
gradient_accumulation_steps=8

# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
model.gradient_checkpointing_enable()
```

#### 2. æ¨¡å‹å­˜å–è¢«æ‹’çµ•
```
401 Client Error: Unauthorized
```

**è§£æ±ºæ–¹æ¡ˆ**:
1. ç¢ºèªå·²ç”³è«‹ MedGemma æ¨¡å‹å­˜å–æ¬Šé™
2. æª¢æŸ¥ HuggingFace token æ˜¯å¦æ­£ç¢º
3. é‡æ–°åŸ·è¡Œèªè­‰æµç¨‹

#### 3. å¥—ä»¶ç‰ˆæœ¬è¡çª
```
ImportError: cannot import name 'xxx'
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ›´æ–°åˆ°æŒ‡å®šç‰ˆæœ¬
pip install transformers==4.44.0

# é‡æ–°å®‰è£è¡çªå¥—ä»¶
pip install --force-reinstall peft

# ä½¿ç”¨è™›æ“¬ç’°å¢ƒ
conda create -n medgemma python=3.10
```

#### 4. æ•¸æ“šæ ¼å¼éŒ¯èª¤
```
KeyError: 'text'
```

**è§£æ±ºæ–¹æ¡ˆ**:
1. æª¢æŸ¥ CSV æª”æ¡ˆæ ¼å¼
2. ç¢ºèªæ¬„ä½åç¨±æ­£ç¢º
3. æª¢æŸ¥æ•¸æ“šé è™•ç†æ­¥é©Ÿ

### ğŸ” èª¿è©¦æŠ€å·§

#### 1. GPU è¨˜æ†¶é«”ç›£æ§
```bash
# å³æ™‚ç›£æ§ GPU ä½¿ç”¨æƒ…æ³
nvidia-smi -l 1

# åœ¨ Python ä¸­æª¢æŸ¥è¨˜æ†¶é«”
import torch
print(f"å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"å·²ä¿ç•™: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

#### 2. è¨“ç·´éç¨‹ç›£æ§
```bash
# å•Ÿå‹• TensorBoard
tensorboard --logdir ./results

# æŸ¥çœ‹è¨“ç·´æ—¥èªŒ
tail -f ./results/trainer_state.json
```

#### 3. æ¨¡å‹é©—è­‰
```python
# æª¢æŸ¥æ¨¡å‹åƒæ•¸
print(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
print(f"ç¸½åƒæ•¸: {all_params:,}")
print(f"åƒæ•¸æ¯”ä¾‹: {100 * trainable_params / all_params:.2f}%")

# æª¢æŸ¥æ•¸æ“šé›†
print(f"æ•¸æ“šé›†å¤§å°: {len(dataset)}")
print(f"æ¨£æœ¬ç¯„ä¾‹: {dataset[0]}")
```

---

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–

### ğŸ¯ è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥

#### 1. é‡åŒ–å„ªåŒ–
```python
# ä½¿ç”¨ 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

#### 2. æ¢¯åº¦å„ªåŒ–
```python
# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
model.gradient_checkpointing_enable()

# è¨­ç½®æ¢¯åº¦ç´¯ç©
gradient_accumulation_steps=4

# æ¢¯åº¦è£å‰ª
max_grad_norm=0.3
```

#### 3. æ‰¹æ¬¡è™•ç†å„ªåŒ–
```python
# å‹•æ…‹å¡«å……
pad_to_multiple_of=8

# æŒ‰é•·åº¦åˆ†çµ„
group_by_length=True

# æ··åˆç²¾åº¦è¨“ç·´
bf16=True
```

### âš¡ é€Ÿåº¦å„ªåŒ–ç­–ç•¥

#### 1. æ•¸æ“šè¼‰å…¥å„ªåŒ–
```python
# è¨­ç½®æ•¸æ“šè¼‰å…¥å™¨
dataloader_pin_memory=False
remove_unused_columns=False

# ä½¿ç”¨å¤šé€²ç¨‹è¼‰å…¥
num_workers=4
```

#### 2. æ¨¡å‹å„ªåŒ–
```python
# ä½¿ç”¨æ›´å¿«çš„å„ªåŒ–å™¨
optim="paged_adamw_32bit"

# æ¸›å°‘æ—¥èªŒé »ç‡
logging_steps=25
```

#### 3. ç¡¬é«”å„ªåŒ–
```python
# ä½¿ç”¨ BF16 ç²¾åº¦
bf16=True

# å•Ÿç”¨ CUDA åœ–
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é …

### ğŸ¥ é†«ç™‚å…è²¬è²æ˜

- **æ•™è‚²ç”¨é€”**: æ­¤æ¨¡å‹åƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”
- **å°ˆæ¥­é©—è­‰**: é†«ç™‚å»ºè­°æ‡‰ç”±åˆæ ¼é†«ç™‚å°ˆæ¥­äººå“¡é©—è­‰
- **è‡¨åºŠé™åˆ¶**: ä¸æ‡‰ç›´æ¥ç”¨æ–¼è‡¨åºŠè¨ºæ–·

### ğŸ”’ æ•¸æ“šéš±ç§

- **é©ç•¶ä½¿ç”¨**: ç¢ºä¿ä½¿ç”¨é©ç•¶çš„é†«ç™‚æ•¸æ“š
- **æ³•è¦éµå®ˆ**: éµå®ˆç›¸é—œéš±ç§æ³•è¦
- **å»è­˜åˆ¥åŒ–**: æ³¨æ„æ•¸æ“šå»è­˜åˆ¥åŒ–è™•ç†

### ğŸ¯ æ¨¡å‹é™åˆ¶

- **åè¦‹é¢¨éšª**: å¯èƒ½å­˜åœ¨åè¦‹å’Œä¸æº–ç¢ºæ€§
- **æŒçºŒè©•ä¼°**: éœ€è¦æŒçºŒè©•ä¼°å’Œæ”¹é€²
- **å®‰å…¨æª¢æŸ¥**: å»ºè­°åŠ å…¥å®‰å…¨æª¢æŸ¥æ©Ÿåˆ¶

---

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤ Issue å’Œ Pull Request ä¾†æ”¹é€²é€™å€‹å°ˆæ¡ˆï¼

### å¦‚ä½•è²¢ç»

1. **Fork å°ˆæ¡ˆ**: åœ¨ GitHub ä¸Š Fork æ­¤å°ˆæ¡ˆ
2. **å‰µå»ºåˆ†æ”¯**: å‰µå»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. **æäº¤è®Šæ›´**: æäº¤æ‚¨çš„è®Šæ›´ (`git commit -m 'Add some AmazingFeature'`)
4. **æ¨é€åˆ†æ”¯**: æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. **ç™¼èµ· PR**: é–‹å•Ÿ Pull Request

### è²¢ç»æŒ‡å—

- è«‹ç¢ºä¿ä»£ç¢¼ç¬¦åˆ PEP 8 è¦ç¯„
- æ·»åŠ é©ç•¶çš„è¨»é‡‹å’Œæ–‡æª”
- æ¸¬è©¦æ‚¨çš„æ›´æ”¹
- æ›´æ–°ç›¸é—œæ–‡æª”

---

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆåƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”ã€‚è«‹å°Šé‡åŸå§‹æ¨¡å‹å’Œæ•¸æ“šé›†çš„æˆæ¬Šæ¢æ¬¾ã€‚

- **MedGemma æ¨¡å‹**: éµå¾ª Google çš„æˆæ¬Šæ¢æ¬¾
- **MedQuAD æ•¸æ“šé›†**: éµå¾ªåŸå§‹æ•¸æ“šé›†çš„æˆæ¬Šæ¢æ¬¾
- **æœ¬å°ˆæ¡ˆ**: MIT License

---

## ğŸ“ æ”¯æ´

å¦‚æœæ‚¨é‡åˆ°å•é¡Œæˆ–æœ‰ç–‘å•ï¼Œè«‹ï¼š

1. **æŸ¥çœ‹æ–‡æª”**: ä»”ç´°é–±è®€æœ¬ README å’Œç›¸é—œæ–‡æª”
2. **æœå°‹ Issue**: åœ¨ GitHub Issues ä¸­æœå°‹é¡ä¼¼å•é¡Œ
3. **å‰µå»º Issue**: å¦‚æœæ²’æœ‰æ‰¾åˆ°è§£æ±ºæ–¹æ¡ˆï¼Œè«‹å‰µå»ºæ–°çš„ Issue
4. **æä¾›è³‡è¨Š**: åœ¨ Issue ä¸­æä¾›è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯å’Œç’°å¢ƒé…ç½®

### è¯ç¹«æ–¹å¼

- **GitHub Issues**: [å‰µå»º Issue](https://github.com/Kiwi1009/Fine-tune-MedGemma/issues)
- **Email**: é€šé GitHub å€‹äººè³‡æ–™è¯ç¹«

---

<div align="center">

**â­ å¦‚æœé€™å€‹å°ˆæ¡ˆå°æ‚¨æœ‰å¹«åŠ©ï¼Œè«‹çµ¦æˆ‘å€‘ä¸€å€‹ Starï¼**

**ğŸš€ ç¥æ‚¨è¨“ç·´é †åˆ©ï¼**

</div> 