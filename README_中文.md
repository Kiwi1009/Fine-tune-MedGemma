# MedGemma 27B ä¸­æ–‡å¾®èª¿æŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

é€™å€‹å°ˆæ¡ˆå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LoRA (Low-Rank Adaptation) æŠ€è¡“åœ¨é†«ç™‚å•ç­”æ•¸æ“šä¸Šå¾®èª¿ MedGemma 27B å¤šæ¨¡æ…‹æ¨¡å‹ã€‚å°ˆæ¡ˆé‡å° GPU è¨“ç·´é€²è¡Œäº†å„ªåŒ–ï¼Œç‰¹åˆ¥é©åˆ RTX 4090 ç­‰é«˜ç«¯é¡¯å¡ã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

- **GPU åŠ é€Ÿè¨“ç·´**: æ”¯æ´ CUDA GPU åŠ é€Ÿï¼Œå„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨
- **LoRA å¾®èª¿**: é«˜æ•ˆåƒæ•¸é©æ‡‰ï¼Œç„¡éœ€å®Œæ•´æ¨¡å‹é‡è¨“ç·´
- **é†«ç™‚å°ˆç”¨**: åŸºæ–¼ MedQuAD é†«ç™‚å•ç­”æ•¸æ“šé›†è¨“ç·´
- **è¨˜æ†¶é«”ç›£æ§**: å…§å»º GPU è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤å’Œå„ªåŒ–å»ºè­°
- **ä¸­æ–‡æ”¯æ´**: å®Œæ•´çš„ä¸­æ–‡èªªæ˜å’ŒéŒ¯èª¤è™•ç†

## ğŸ–¥ï¸ ç³»çµ±éœ€æ±‚

### ç¡¬é«”éœ€æ±‚
- **GPU**: NVIDIA GPUï¼Œè‡³å°‘ 24GB VRAM (æ¨è–¦ RTX 4090 æˆ– A100)
- **è¨˜æ†¶é«”**: è‡³å°‘ 32GB ç³»çµ±è¨˜æ†¶é«”
- **å„²å­˜**: è‡³å°‘ 50GB å¯ç”¨ç©ºé–“

### è»Ÿé«”éœ€æ±‚
- **ä½œæ¥­ç³»çµ±**: Windows 10/11, Linux, æˆ– macOS
- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **CUDA**: 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬ (èˆ‡ PyTorch ç›¸å®¹)

## ğŸš€ å®‰è£æŒ‡å—

### 1. ç’°å¢ƒæº–å‚™

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ (æ¨è–¦)
conda create -n medgemma python=3.10
conda activate medgemma

# æˆ–ä½¿ç”¨ venv
python -m venv medgemma_env
source medgemma_env/bin/activate  # Linux/Mac
# æˆ–
medgemma_env\Scripts\activate  # Windows
```

### 2. å®‰è£å¿…è¦å¥—ä»¶

```bash
# å®‰è£ PyTorch (æ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬é¸æ“‡)
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£å…¶ä»–å¿…è¦å¥—ä»¶
pip install transformers==4.44.0
pip install peft
pip install bitsandbytes
pip install accelerate
pip install datasets
pip install pandas
pip install huggingface_hub
pip install jupyter
pip install tensorboard
```

### 3. é©—è­‰å®‰è£

```python
import torch
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU å‹è™Ÿ: {torch.cuda.get_device_name(0)}")
    print(f"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

## ğŸ” æ¨¡å‹å­˜å–è¨­ç½®

### 1. HuggingFace å¸³è™Ÿè¨­ç½®

1. å‰å¾€ [HuggingFace](https://huggingface.co/) è¨»å†Šå¸³è™Ÿ
2. å‰å¾€ [MedGemma æ¨¡å‹é é¢](https://huggingface.co/google/medgemma-4b-multimodal)
3. é»æ“Š "Request Access" ç”³è«‹å­˜å–æ¬Šé™
4. æ¥å—æˆæ¬Šå”è­°
5. ç­‰å¾…å¯©æ ¸é€šé (é€šå¸¸éœ€è¦å¹¾åˆ†é˜åˆ°å¹¾å°æ™‚)

### 2. å–å¾— API Token

1. å‰å¾€ [Token è¨­ç½®é é¢](https://huggingface.co/settings/tokens)
2. é»æ“Š "New token"
3. é¸æ“‡ "Read" æ¬Šé™
4. è¤‡è£½ç”Ÿæˆçš„ token

### 3. é©—è­‰å­˜å–

```python
from huggingface_hub import login
login(token="your_token_here")

# æ¸¬è©¦æ¨¡å‹å­˜å–
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google/medgemma-4b-multimodal")
print("âœ… æ¨¡å‹å­˜å–æˆåŠŸ!")
```

## ğŸ“Š æ•¸æ“šé›†æº–å‚™

### 1. MedQuAD æ•¸æ“šé›†

å°ˆæ¡ˆä½¿ç”¨ MedQuAD é†«ç™‚å•ç­”æ•¸æ“šé›†ï¼ŒåŒ…å«ï¼š
- **å•é¡Œ**: é†«ç™‚ç›¸é—œå•é¡Œ
- **ç­”æ¡ˆ**: å°ˆæ¥­é†«ç™‚å›ç­”
- **ä¾†æº**: å¯é çš„é†«ç™‚è³‡æº
- **é ˜åŸŸ**: æ¶µè“‹å¤šå€‹é†«ç™‚å°ˆæ¥­é ˜åŸŸ

### 2. æ•¸æ“šæ ¼å¼

æ•¸æ“šæ‡‰ç‚º CSV æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
```csv
question,answer,source,focus_area
"ä»€éº¼æ˜¯ç³–å°¿ç—…?","ç³–å°¿ç—…æ˜¯ä¸€ç¨®ä»£è¬ç–¾ç—…...",medical_textbook,endocrinology
```

### 3. æ•¸æ“šé è™•ç†

```python
import pandas as pd

# è¼‰å…¥æ•¸æ“š
df = pd.read_csv('medquad.csv')

# æ ¼å¼åŒ–ç‚ºæŒ‡ä»¤-å›æ‡‰æ ¼å¼
def format_instruction(row):
    return f"Instruction: {row['question']}\nResponse: {row['answer']}"

df['text'] = df.apply(format_instruction, axis=1)
```

## ğŸ›ï¸ æ¨¡å‹é…ç½®

### 1. é‡åŒ–è¨­ç½® (4-bit)

```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
```

### 2. LoRA é…ç½®

```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,                    # LoRA ç§©
    lora_alpha=32,           # ç¸®æ”¾åƒæ•¸
    lora_dropout=0.05,       # Dropout ç‡
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "v_proj"],  # ç›®æ¨™æ¨¡çµ„
)
```

### 3. è¨“ç·´åƒæ•¸

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    weight_decay=0.001,
    bf16=True,               # ä½¿ç”¨ BF16 ç²¾åº¦
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    save_steps=100,
    logging_steps=25,
    report_to="tensorboard",
)
```

## ğŸš€ é–‹å§‹è¨“ç·´

### 1. åŸ·è¡Œè¨“ç·´è…³æœ¬

```bash
# ä½¿ç”¨ Jupyter Notebook
jupyter notebook finetune_medgemma_27b_ä¸­æ–‡.ipynb

# æˆ–ä½¿ç”¨ Python è…³æœ¬
python train_medgemma_27b.py
```

### 2. ç›£æ§è¨“ç·´éç¨‹

```bash
# ç›£æ§ GPU ä½¿ç”¨æƒ…æ³
nvidia-smi -l 1

# å•Ÿå‹• TensorBoard
tensorboard --logdir ./results
```

### 3. è¨“ç·´æª¢æŸ¥é»

è¨“ç·´éç¨‹ä¸­æœƒè‡ªå‹•ä¿å­˜ï¼š
- **æ¨¡å‹æ¬Šé‡**: LoRA é©é…å™¨
- **é…ç½®æª”æ¡ˆ**: è¨“ç·´åƒæ•¸å’Œæ¨¡å‹é…ç½®
- **æ—¥èªŒæª”æ¡ˆ**: è¨“ç·´éç¨‹è¨˜éŒ„

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–

### 1. è¨˜æ†¶é«”å„ªåŒ–

- **æ‰¹æ¬¡å¤§å°**: æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
- **åºåˆ—é•·åº¦**: æ§åˆ¶è¼¸å…¥é•·åº¦ä»¥ç¯€çœè¨˜æ†¶é«”
- **æ¢¯åº¦ç´¯ç©**: ç¶­æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°

### 2. é€Ÿåº¦å„ªåŒ–

- **æ··åˆç²¾åº¦**: ä½¿ç”¨ BF16 åŠ é€Ÿè¨“ç·´
- **æ¢¯åº¦æª¢æŸ¥é»**: ç¯€çœè¨˜æ†¶é«”
- **æ•¸æ“šè¼‰å…¥**: å„ªåŒ–æ•¸æ“šç®¡é“

### 3. å¸¸è¦‹å•é¡Œè§£æ±º

#### è¨˜æ†¶é«”ä¸è¶³ (OOM)
```python
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size=2

# æ¸›å°‘åºåˆ—é•·åº¦
max_length=256

# å¢åŠ æ¢¯åº¦ç´¯ç©
gradient_accumulation_steps=8
```

#### è¨“ç·´é€Ÿåº¦æ…¢
```python
# å¢åŠ æ‰¹æ¬¡å¤§å° (å¦‚æœè¨˜æ†¶é«”å…è¨±)
per_device_train_batch_size=8

# ä½¿ç”¨æ›´å¿«çš„å„ªåŒ–å™¨
optim="adamw_torch"

# æ¸›å°‘æ—¥èªŒé »ç‡
logging_steps=50
```

## ğŸ§ª æ¨¡å‹æ¸¬è©¦

### 1. è¼‰å…¥å¾®èª¿æ¨¡å‹

```python
from peft import PeftModel

# è¼‰å…¥åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "google/medgemma-4b-multimodal",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# è¼‰å…¥ LoRA é©é…å™¨
finetuned_model = PeftModel.from_pretrained(base_model, "./finetuned_medgemma_4b")
```

### 2. ç”Ÿæˆé†«ç™‚å›ç­”

```python
def generate_medical_response(question, max_length=256):
    test_input = f"Instruction: {question}\nResponse:"
    inputs = tokenizer(test_input, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = finetuned_model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("Response:")[-1].strip()

# æ¸¬è©¦å•é¡Œ
questions = [
    "ä»€éº¼æ˜¯ç³–å°¿ç—…çš„ç—‡ç‹€?",
    "é«˜è¡€å£“å¦‚ä½•æ²»ç™‚?",
    "å¿ƒè‡Ÿç—…çš„åŸå› æ˜¯ä»€éº¼?",
]

for question in questions:
    answer = generate_medical_response(question)
    print(f"å•é¡Œ: {question}")
    print(f"å›ç­”: {answer}\n")
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
Fine-tune-MedGemma/
â”œâ”€â”€ finetune_medgemma_27b_ä¸­æ–‡.ipynb    # ä¸»è¦è¨“ç·´è…³æœ¬
â”œâ”€â”€ train_medgemma_27b.py              # Python è¨“ç·´è…³æœ¬
â”œâ”€â”€ medquad.csv                        # é†«ç™‚å•ç­”æ•¸æ“šé›†
â”œâ”€â”€ README_ä¸­æ–‡.md                     # ä¸­æ–‡èªªæ˜æ–‡ä»¶
â”œâ”€â”€ requirements.txt                    # ä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ results/                           # è¨“ç·´çµæœç›®éŒ„
â”‚   â”œâ”€â”€ checkpoints/                   # æ¨¡å‹æª¢æŸ¥é»
â”‚   â””â”€â”€ logs/                         # è¨“ç·´æ—¥èªŒ
â””â”€â”€ finetuned_medgemma_4b/            # å¾®èª¿æ¨¡å‹è¼¸å‡º
    â”œâ”€â”€ adapter_config.json            # LoRA é…ç½®
    â”œâ”€â”€ adapter_model.bin              # LoRA æ¬Šé‡
    â””â”€â”€ training_config.json           # è¨“ç·´é…ç½®
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹éŒ¯èª¤åŠè§£æ±ºæ–¹æ¡ˆ

#### 1. CUDA è¨˜æ†¶é«”ä¸è¶³
```
RuntimeError: CUDA out of memory
```
**è§£æ±ºæ–¹æ¡ˆ**:
- æ¸›å°‘ `per_device_train_batch_size`
- æ¸›å°‘ `max_length`
- å¢åŠ  `gradient_accumulation_steps`

#### 2. æ¨¡å‹å­˜å–è¢«æ‹’çµ•
```
401 Client Error: Unauthorized
```
**è§£æ±ºæ–¹æ¡ˆ**:
- ç¢ºèªå·²ç”³è«‹ MedGemma æ¨¡å‹å­˜å–æ¬Šé™
- æª¢æŸ¥ HuggingFace token æ˜¯å¦æ­£ç¢º
- é‡æ–°åŸ·è¡Œ `login()` å‡½æ•¸

#### 3. å¥—ä»¶ç‰ˆæœ¬è¡çª
```
ImportError: cannot import name 'xxx'
```
**è§£æ±ºæ–¹æ¡ˆ**:
- æ›´æ–°åˆ°æŒ‡å®šç‰ˆæœ¬: `pip install transformers==4.44.0`
- é‡æ–°å®‰è£è¡çªå¥—ä»¶
- ä½¿ç”¨è™›æ“¬ç’°å¢ƒéš”é›¢ä¾è³´

#### 4. æ•¸æ“šæ ¼å¼éŒ¯èª¤
```
KeyError: 'text'
```
**è§£æ±ºæ–¹æ¡ˆ**:
- æª¢æŸ¥ CSV æª”æ¡ˆæ ¼å¼
- ç¢ºèªæ¬„ä½åç¨±æ­£ç¢º
- æª¢æŸ¥æ•¸æ“šé è™•ç†æ­¥é©Ÿ

## ğŸ“Š æ€§èƒ½åŸºæº–

### ç¡¬é«”é…ç½®å»ºè­°

| GPU å‹è™Ÿ | VRAM | æ‰¹æ¬¡å¤§å° | åºåˆ—é•·åº¦ | è¨“ç·´æ™‚é–“ |
|----------|------|----------|----------|----------|
| RTX 4090 | 24GB | 4 | 512 | ~2å°æ™‚ |
| RTX 3090 | 24GB | 2 | 512 | ~4å°æ™‚ |
| RTX 3080 | 10GB | 1 | 256 | ~8å°æ™‚ |

### è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

- **æ¨¡å‹è¼‰å…¥**: ~8GB
- **è¨“ç·´éç¨‹**: ~18-22GB
- **ç·©è¡å€**: ~2-4GB

## âš ï¸ é‡è¦æ³¨æ„äº‹é …

### 1. é†«ç™‚å…è²¬è²æ˜
- æ­¤æ¨¡å‹åƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”
- é†«ç™‚å»ºè­°æ‡‰ç”±åˆæ ¼é†«ç™‚å°ˆæ¥­äººå“¡é©—è­‰
- ä¸æ‡‰ç›´æ¥ç”¨æ–¼è‡¨åºŠè¨ºæ–·

### 2. æ•¸æ“šéš±ç§
- ç¢ºä¿ä½¿ç”¨é©ç•¶çš„é†«ç™‚æ•¸æ“š
- éµå®ˆç›¸é—œéš±ç§æ³•è¦
- æ³¨æ„æ•¸æ“šå»è­˜åˆ¥åŒ–

### 3. æ¨¡å‹é™åˆ¶
- å¯èƒ½å­˜åœ¨åè¦‹å’Œä¸æº–ç¢ºæ€§
- éœ€è¦æŒçºŒè©•ä¼°å’Œæ”¹é€²
- å»ºè­°åŠ å…¥å®‰å…¨æª¢æŸ¥æ©Ÿåˆ¶

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤ Issue å’Œ Pull Request ä¾†æ”¹é€²é€™å€‹å°ˆæ¡ˆï¼

### å¦‚ä½•è²¢ç»
1. Fork é€™å€‹å°ˆæ¡ˆ
2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤è®Šæ›´
4. ç™¼èµ· Pull Request

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆåƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç”¨é€”ã€‚è«‹å°Šé‡åŸå§‹æ¨¡å‹å’Œæ•¸æ“šé›†çš„æˆæ¬Šæ¢æ¬¾ã€‚

## ğŸ“ æ”¯æ´

å¦‚æœæ‚¨é‡åˆ°å•é¡Œæˆ–æœ‰ç–‘å•ï¼Œè«‹ï¼š
1. æŸ¥çœ‹æœ¬æ–‡ä»¶çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æœå°‹ç¾æœ‰çš„ Issue
3. å‰µå»ºæ–°çš„ Issue ä¸¦æä¾›è©³ç´°è³‡è¨Š

---

**ç¥æ‚¨è¨“ç·´é †åˆ©ï¼** ğŸš€ 