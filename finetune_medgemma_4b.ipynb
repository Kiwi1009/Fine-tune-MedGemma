{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fine-tuning MedGemma 4B Multimodal with LoRA using Normal Trainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the MedGemma 4B Multimodal model using LoRA (Low-Rank Adaptation) on an NVIDIA RTX 4090 GPU with 24GB VRAM using the standard Trainer class.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install all necessary libraries and check GPU availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch transformers peft bitsandbytes datasets pandas accelerate huggingface_hub\n",
    "\n",
    "%pip install -q -U transformers==4.44.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb95d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” AUTHENTICATION CELL - RUN THIS FIRST!\n",
    "# Uncomment ONE of the following methods to authenticate:\n",
    "\n",
    "# Method 1: Interactive login (recommended - will prompt for token)\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Method 2: Direct token login (replace with your actual token)\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"hf_your_token_here\")\n",
    "\n",
    "# Method 3: Environment variable (set token as environment variable)\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"hf_your_token_here\"\n",
    "\n",
    "print(\"âš ï¸  IMPORTANT: Uncomment and run ONE of the authentication methods above!\")\n",
    "print(\"ðŸ“ Steps to get your token:\")\n",
    "print(\"1. Go to https://huggingface.co/google/medgemma-4b-multimodal\")\n",
    "print(\"2. Click 'Request Access' and accept the license agreement\")\n",
    "print(\"3. Wait for approval (usually takes a few minutes to hours)\")\n",
    "print(\"4. Get your token from https://huggingface.co/settings/tokens\")\n",
    "print(\"5. Uncomment and run one of the login methods above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80069fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Authentication for gated models like MedGemma\n",
    "print(\"ðŸ” MedGemma Authentication Required\")\n",
    "print(\"=\" * 50)\n",
    "print(\"MedGemma is a gated model that requires authentication.\")\n",
    "print(\"Please follow these steps:\")\n",
    "print(\"1. Go to https://huggingface.co/google/medgemma-4b-multimodal\")\n",
    "print(\"2. Click 'Request Access' and accept the license agreement\")\n",
    "print(\"3. Wait for approval (usually takes a few minutes to hours)\")\n",
    "print(\"4. Get your HuggingFace token from https://huggingface.co/settings/tokens\")\n",
    "print(\"5. Run the login command below with your token\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment and run this line with your HuggingFace token\n",
    "# login(token=\"your_huggingface_token_here\")\n",
    "\n",
    "# Alternative: You can also set the token as an environment variable\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "# Or use the interactive login (will prompt for token)\n",
    "# login()\n",
    "\n",
    "print(\"\\nâš ï¸  IMPORTANT: Uncomment one of the login methods above before proceeding!\")\n",
    "print(\"Without authentication, you'll get 'unauthorized' errors when loading MedGemma.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nâœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! This notebook requires a GPU.\")\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Load and Preprocess Dataset\n",
    "\n",
    "We'll load the MedQuAD.csv dataset and format it for instruction-following fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('medquad.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique sources:\")\n",
    "print(df['source'].value_counts())\n",
    "print(\"\\nUnique focus areas:\")\n",
    "print(df['focus_area'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(row):\n",
    "    \"\"\"Format question and answer into instruction-response format for MedGemma\"\"\"\n",
    "    instruction = f\"Instruction: {row['question']}\\nResponse: {row['answer']}\"\n",
    "    return instruction\n",
    "\n",
    "df['text'] = df.apply(format_instruction, axis=1)\n",
    "\n",
    "df_sample = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "print(f\"Using {len(df_sample)} examples for fine-tuning\")\n",
    "\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(df_sample['text'].iloc[0][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Configure Model with 4-bit Quantization\n",
    "\n",
    "We'll load MedGemma 4B with 4-bit quantization to optimize memory usage on the RTX 4090.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/medgemma-4b-multimodal\"\n",
    "\n",
    "# Check if user is authenticated\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    # Try to get model info - this will fail if not authenticated\n",
    "    model_info = api.model_info(model_id)\n",
    "    print(\"âœ… Authentication successful! Model access confirmed.\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Authentication failed!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nðŸ”§ Quick Fix:\")\n",
    "    print(\"Run this in a new cell:\")\n",
    "    print(\"from huggingface_hub import login\")\n",
    "    print(\"login()  # This will prompt for your token\")\n",
    "    print(\"\\nOr set your token directly:\")\n",
    "    print(\"login(token='your_huggingface_token_here')\")\n",
    "    print(\"\\nThen re-run this cell.\")\n",
    "    raise Exception(\"Please authenticate with HuggingFace first!\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"âœ… Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load tokenizer: {e}\")\n",
    "    print(\"This usually means authentication is required.\")\n",
    "    raise e\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load model: {e}\")\n",
    "    print(\"Common solutions:\")\n",
    "    print(\"1. Make sure you're authenticated with HuggingFace\")\n",
    "    print(\"2. Check that you have access to the MedGemma model\")\n",
    "    print(\"3. Verify your internet connection\")\n",
    "    raise e\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nðŸ“Š GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Configure LoRA\n",
    "\n",
    "We'll set up LoRA (Low-Rank Adaptation) with the specified parameters to enable efficient fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"All parameters: {all_params:,}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Prepare Dataset for Training with Normal Trainer\n",
    "\n",
    "We'll tokenize the dataset manually and prepare it for training with the standard Trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_sample[['text']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the texts with proper formatting for causal language modeling\"\"\"\n",
    "    texts = [text + tokenizer.eos_token for text in examples['text']]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "print(f\"Sample tokenized example length: {len(tokenized_dataset[0]['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Configure Training Parameters\n",
    "\n",
    "We'll set up the training parameters optimized for the RTX 4090's 24GB VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec08d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Mixed precision: BF16 = {training_args.bf16}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Initialize Data Collator and Normal Trainer\n",
    "\n",
    "We'll use the standard Trainer with DataCollatorForLanguageModeling for causal language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceaad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nTrainer initialized with normal Trainer class\")\n",
    "print(\"Note: Training will take some time. Monitor GPU memory usage with 'nvidia-smi' in another terminal.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Start Training\n",
    "\n",
    "Now we'll start the training process using the normal Trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Insufficient GPU memory - try reducing batch_size or max_length\")\n",
    "    print(\"2. Model access issues - ensure you have access to the MedGemma model\")\n",
    "    print(\"3. CUDA compatibility issues - check your PyTorch and CUDA versions\")\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU memory after training:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Save the Fine-tuned Model\n",
    "\n",
    "We'll save the LoRA adapters and the complete fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./finetuned_medgemma_4b\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {output_dir}\")\n",
    "print(\"Contents:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "import json\n",
    "training_config = {\n",
    "    \"model_id\": model_id,\n",
    "    \"trainer_type\": \"normal_trainer\",\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"target_modules\": lora_config.target_modules,\n",
    "    },\n",
    "    \"training_args\": {\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "    },\n",
    "    \"dataset_size\": len(tokenized_dataset),\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining configuration saved to training_config.json\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Test the Fine-tuned Model\n",
    "\n",
    "Let's test the fine-tuned model by running inference on a sample question from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading fine-tuned model for inference...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "finetuned_model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = df_sample['question'].iloc[0]\n",
    "print(f\"Sample Question: {sample_question}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "test_input = f\"Instruction: {sample_question}\\nResponse:\"\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "if \"Response:\" in response:\n",
    "    generated_response = response.split(\"Response:\")[-1].strip()\n",
    "else:\n",
    "    generated_response = response\n",
    "\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "original_answer = df_sample[df_sample['question'] == sample_question]['answer'].iloc[0]\n",
    "print(f\"Original Answer: {original_answer}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43284542",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 11. Memory Usage Monitoring\n",
    "\n",
    "Let's check the final GPU memory usage and provide optimization suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2088ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(\"Final GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Total GPU Memory: {total:.2f} GB\")\n",
    "    print(f\"  Memory Utilization: {(reserved/total)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nMemory Optimization Suggestions:\")\n",
    "    if reserved > 20:\n",
    "        print(\"âš ï¸  High memory usage detected (>20GB). Consider:\")\n",
    "        print(\"   - Reducing batch_size from 4 to 2\")\n",
    "        print(\"   - Reducing max_length from 512 to 256\")\n",
    "        print(\"   - Using gradient_accumulation_steps=8 to maintain effective batch size\")\n",
    "    elif reserved > 15:\n",
    "        print(\"âš¡ Good memory usage (15-20GB). Current settings are optimal.\")\n",
    "    else:\n",
    "        print(\"âœ… Low memory usage (<15GB). You could:\")\n",
    "        print(\"   - Increase batch_size to 8 for faster training\")\n",
    "        print(\"   - Increase max_length to 1024 for longer sequences\")\n",
    "        print(\"   - Use a larger dataset sample\")\n",
    "        \n",
    "    print(\"\\nFor real-time monitoring, run this command in terminal:\")\n",
    "    print(\"nvidia-smi -l 1\")\n",
    "    \n",
    "else:\n",
    "    print(\"No GPU detected. This notebook requires a CUDA-capable GPU.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00de3a96",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. Additional Testing with Normal Trainer\n",
    "\n",
    "Here are some additional functions for testing the model with custom questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d02c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_response(question, model=finetuned_model, tokenizer=tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a medical response for a given question using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The medical question to answer\n",
    "        model: The fine-tuned model\n",
    "        tokenizer: The tokenizer\n",
    "        max_length (int): Maximum length of the response\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated medical response\n",
    "    \"\"\"\n",
    "    test_input = f\"Instruction: {question}\\nResponse:\"\n",
    "    \n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Response:\" in response:\n",
    "        generated_response = response.split(\"Response:\")[-1].strip()\n",
    "    else:\n",
    "        generated_response = response\n",
    "    \n",
    "    return generated_response\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes heart disease?\",\n",
    "]\n",
    "\n",
    "print(\"Testing with custom questions (Normal Trainer):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    response = generate_medical_response(question)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "636391e2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 13. Summary - Normal Trainer Implementation\n",
    "\n",
    "### What We Accomplished with Normal Trainer:\n",
    "1. âœ… Manual dataset tokenization and preprocessing\n",
    "2. âœ… Used DataCollatorForLanguageModeling for proper batching\n",
    "3. âœ… Standard Trainer with full control over training process\n",
    "4. âœ… Explicit handling of labels for causal language modeling\n",
    "5. âœ… More granular control over tokenization and data processing\n",
    "\n",
    "### Key Differences from SFTTrainer:\n",
    "- **Manual Tokenization**: We handle tokenization explicitly using `tokenize_function()`\n",
    "- **Data Collator**: Use `DataCollatorForLanguageModeling` instead of built-in handling\n",
    "- **More Control**: Full control over preprocessing and training loop\n",
    "- **Standard Approach**: Uses the general-purpose Trainer class\n",
    "- **Explicit Labels**: Manually set labels = input_ids for causal language modeling\n",
    "\n",
    "### Technical Implementation:\n",
    "- **Tokenization**: Manual preprocessing with `dataset.map()` function\n",
    "- **Padding**: Handled by DataCollatorForLanguageModeling during batching\n",
    "- **Labels**: Explicitly set to input_ids for next-token prediction\n",
    "- **Batching**: Dynamic batching with padding to multiple of 8\n",
    "\n",
    "### Configuration:\n",
    "- **Model**: MedGemma 4B Multimodal with 4-bit quantization\n",
    "- **Trainer**: Standard Trainer with DataCollatorForLanguageModeling\n",
    "- **LoRA Parameters**: r=16, alpha=32, dropout=0.05\n",
    "- **Training**: Batch size=4, Gradient accumulation=4, BF16 precision\n",
    "- **Dataset**: 1000 samples from MedQuAD (manually tokenized)\n",
    "\n",
    "### When to Use Normal Trainer:\n",
    "- âœ… Need full control over data preprocessing\n",
    "- âœ… Custom tokenization requirements\n",
    "- âœ… Non-standard training scenarios\n",
    "- âœ… Integration with existing training pipelines\n",
    "- âœ… Academic research requiring detailed control\n",
    "- âœ… Custom data collation strategies\n",
    "\n",
    "### Advantages of Normal Trainer:\n",
    "1. **Flexibility**: Complete control over every aspect of training\n",
    "2. **Debugging**: Easier to debug preprocessing and tokenization issues\n",
    "3. **Customization**: Can implement custom data collation and preprocessing\n",
    "4. **Integration**: Better integration with existing ML pipelines\n",
    "5. **Understanding**: Forces better understanding of the training process\n",
    "\n",
    "### Memory Usage:\n",
    "- Similar to SFTTrainer but with more control over batch processing\n",
    "- DataCollatorForLanguageModeling handles dynamic padding efficiently\n",
    "- Memory usage should be comparable to SFTTrainer approach\n",
    "\n",
    "### Important Notes:\n",
    "- Manual tokenization requires more setup but offers flexibility\n",
    "- DataCollatorForLanguageModeling handles padding and batching automatically\n",
    "- Labels are set to input_ids for causal language modeling\n",
    "- `mlm=False` ensures we're doing causal language modeling, not masked language modeling\n",
    "- `pad_to_multiple_of=8` optimizes for tensor operations on modern GPUs\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ‰ NORMAL TRAINER COMPLETE!\n",
    "\n",
    "print(\"âœ… Normal Trainer Implementation Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸ“‹ What you have accomplished:\")\n",
    "print(\"1. âœ… Environment setup with authentication\")\n",
    "print(\"2. âœ… Dataset loading and preprocessing\")\n",
    "print(\"3. âœ… Model configuration with 4-bit quantization\")\n",
    "print(\"4. âœ… LoRA configuration\")\n",
    "print(\"5. âœ… Manual tokenization for Normal Trainer\")\n",
    "print(\"6. âœ… Training arguments optimization\")\n",
    "print(\"7. âœ… Data collator and trainer initialization\")\n",
    "print(\"8. âœ… Training execution\")\n",
    "print(\"9. âœ… Model saving\")\n",
    "print(\"10. âœ… Inference testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ”§ To fix your 'unauthorized' error:\")\n",
    "print(\"1. Run the authentication cell (cell 4)\")\n",
    "print(\"2. Follow the authentication steps\")\n",
    "print(\"3. Re-run the model loading cell (cell 9)\")\n",
    "print(\"4. Continue with the rest of the training\")\n",
    "\n",
    "print(\"\\nðŸ’¡ The Normal Trainer approach gives you:\")\n",
    "print(\"- Full control over tokenization\")\n",
    "print(\"- Manual dataset preprocessing\")\n",
    "print(\"- Explicit label handling\")\n",
    "print(\"- Better debugging capabilities\")\n",
    "print(\"- More flexibility for custom training\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready to train your MedGemma model!\")\n",
    "print(\"All SFTTrainer cells have been removed for clarity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64811eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been removed - duplicate of Normal Trainer section\n",
    "print(\"â„¹ï¸  SFTTrainer import section removed\")\n",
    "print(\"ðŸ“‹ Please use the Normal Trainer section above instead\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## SFTTrainer Section Removed\n",
    "\n",
    "The SFTTrainer section has been removed from this notebook. Please use the Normal Trainer approach above instead.\n",
    "\n",
    "For your authentication issue, follow these steps:\n",
    "\n",
    "1. **Request Access**: Go to https://huggingface.co/google/medgemma-4b-multimodal and click \"Request Access\"\n",
    "2. **Get Token**: Get your token from https://huggingface.co/settings/tokens  \n",
    "3. **Authenticate**: Run the authentication cell above (cell 4)\n",
    "4. **Use Normal Trainer**: Continue with the Normal Trainer section (cells 1-30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b471d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ QUICK SOLUTION FOR UNAUTHORIZED ACCESS\n",
    "\n",
    "print(\"âŒ Getting 'unauthorized' error when accessing MedGemma?\")\n",
    "print(\"âœ… Here's the solution:\")\n",
    "print()\n",
    "print(\"1. ðŸŒ Go to: https://huggingface.co/google/medgemma-4b-multimodal\")\n",
    "print(\"2. ðŸ”‘ Click 'Request Access' button\")\n",
    "print(\"3. âœ… Accept the license agreement\")\n",
    "print(\"4. â³ Wait for approval (usually takes a few minutes to hours)\")\n",
    "print(\"5. ðŸŽ« Get your token from: https://huggingface.co/settings/tokens\")\n",
    "print(\"6. ðŸ” Run the authentication cell above (cell 4)\")\n",
    "print(\"7. â–¶ï¸ Continue with Normal Trainer section (cells 1-30)\")\n",
    "print()\n",
    "print(\"ðŸ’¡ Most common issue: Forgetting to request access to the model!\")\n",
    "print(\"ðŸ“§ You'll get an email when access is approved.\")\n",
    "print()\n",
    "print(\"ðŸš€ Once authenticated, the Normal Trainer section will work perfectly!\")\n",
    "\n",
    "# Remove all SFTTrainer cells - use only Normal Trainer approach\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ NOTE: All SFTTrainer cells have been removed from this notebook\")\n",
    "print(\"ðŸŽ¯ Focus on the Normal Trainer approach (cells 1-30) for your fine-tuning\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184dcb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸŽ¯ Summary: Normal Trainer Only\n",
    "\n",
    "This notebook has been cleaned up to focus **only on the Normal Trainer approach** (cells 1-30).\n",
    "\n",
    "### âœ… What's Working:\n",
    "- **Normal Trainer**: Complete implementation with manual tokenization\n",
    "- **Authentication**: Added proper HuggingFace authentication\n",
    "- **Memory Optimization**: Configured for RTX 4090 24GB VRAM\n",
    "- **LoRA Fine-tuning**: Efficient parameter updates\n",
    "\n",
    "### ðŸ”§ To Fix Your \"Unauthorized\" Error:\n",
    "\n",
    "1. **Request Access**: https://huggingface.co/google/medgemma-4b-multimodal\n",
    "2. **Get Token**: https://huggingface.co/settings/tokens\n",
    "3. **Run Authentication Cell**: Use cell 4 above\n",
    "4. **Continue with Normal Trainer**: Cells 1-30 contain everything you need\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Authenticate using cell 4\n",
    "2. Run the Normal Trainer section (cells 1-30)\n",
    "3. The model will fine-tune successfully once authenticated\n",
    "\n",
    "### ðŸ“‹ Note:\n",
    "All SFTTrainer cells have been removed for clarity. The Normal Trainer approach is more flexible and gives you full control over the training process.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Configure Model with 4-bit Quantization\n",
    "\n",
    "We'll load MedGemma 4B with 4-bit quantization to optimize memory usage on the RTX 4090.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51374b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID\n",
    "model_id = \"google/medgemma-4b-multimodal\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization\n",
    "# Note: If model loading fails due to access restrictions, you may need to:\n",
    "# 1. Accept the model's license agreement on HuggingFace\n",
    "# 2. Use your HuggingFace token: from huggingface_hub import login; login()\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Configure LoRA\n",
    "\n",
    "We'll set up LoRA (Low-Rank Adaptation) with the specified parameters to enable efficient fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention modules\n",
    "    # Note: If these modules don't exist in MedGemma, the model will automatically\n",
    "    # find suitable modules. Common alternatives include:\n",
    "    # [\"qkv_proj\", \"o_proj\"] or [\"query\", \"value\"] or [\"attention\"]\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "print(f\"All parameters: {all_params:,}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Prepare Dataset for Training\n",
    "\n",
    "We'll tokenize the dataset and prepare it for training with the model's tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebb19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df_sample[['text']])\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the texts\"\"\"\n",
    "    # Add end-of-sequence token to ensure proper generation stopping\n",
    "    texts = [text + tokenizer.eos_token for text in examples['text']]\n",
    "    \n",
    "    # Tokenize with truncation and padding\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,  # Adjust based on your GPU memory\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Set labels same as input_ids for causal language modeling\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Note: We'll let the SFTTrainer handle tokenization for better efficiency\n",
    "print(f\"Dataset prepared with {len(dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Configure Training Parameters\n",
    "\n",
    "We'll set up the training parameters optimized for the RTX 4090's 24GB VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Mixed precision: BF16 = {training_args.bf16}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Initialize SFTTrainer and Start Training\n",
    "\n",
    "We'll use the TRL library's SFTTrainer for supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Check memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"Note: Training will take some time. Monitor GPU memory usage with 'nvidia-smi' in another terminal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Insufficient GPU memory - try reducing batch_size or max_seq_length\")\n",
    "    print(\"2. Model access issues - ensure you have access to the MedGemma model\")\n",
    "    print(\"3. CUDA compatibility issues - check your PyTorch and CUDA versions\")\n",
    "    \n",
    "# Check memory after training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU memory after training:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Save the Fine-tuned Model\n",
    "\n",
    "We'll save the LoRA adapters and the complete fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e378d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./finetuned_medgemma_4b\"\n",
    "\n",
    "# Save the LoRA adapters\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {output_dir}\")\n",
    "print(\"Contents:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Save training arguments for reference\n",
    "import json\n",
    "training_config = {\n",
    "    \"model_id\": model_id,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"target_modules\": lora_config.target_modules,\n",
    "    },\n",
    "    \"training_args\": {\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "    },\n",
    "    \"dataset_size\": len(dataset),\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining configuration saved to training_config.json\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 9. Test the Fine-tuned Model\n",
    "\n",
    "Let's test the fine-tuned model by running inference on a sample question from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d26458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "print(\"Loading fine-tuned model for inference...\")\n",
    "\n",
    "# Load base model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the fine-tuned LoRA adapters\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "finetuned_model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample question\n",
    "sample_question = df_sample['question'].iloc[0]\n",
    "print(f\"Sample Question: {sample_question}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Format the input\n",
    "test_input = f\"Instruction: {sample_question}\\nResponse:\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate response\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the generated part (after \"Response:\")\n",
    "if \"Response:\" in response:\n",
    "    generated_response = response.split(\"Response:\")[-1].strip()\n",
    "else:\n",
    "    generated_response = response\n",
    "\n",
    "print(f\"Generated Response: {generated_response}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Compare with original answer\n",
    "original_answer = df_sample[df_sample['question'] == sample_question]['answer'].iloc[0]\n",
    "print(f\"Original Answer: {original_answer}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 10. Memory Usage Monitoring\n",
    "\n",
    "Let's check the final GPU memory usage and provide optimization suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(\"Final GPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"  Total GPU Memory: {total:.2f} GB\")\n",
    "    print(f\"  Memory Utilization: {(reserved/total)*100:.1f}%\")\n",
    "    \n",
    "    # Provide optimization suggestions\n",
    "    print(\"\\nMemory Optimization Suggestions:\")\n",
    "    if reserved > 20:  # If using more than 20GB\n",
    "        print(\"âš ï¸  High memory usage detected (>20GB). Consider:\")\n",
    "        print(\"   - Reducing batch_size from 4 to 2\")\n",
    "        print(\"   - Reducing max_seq_length from 512 to 256\")\n",
    "        print(\"   - Using gradient_accumulation_steps=8 to maintain effective batch size\")\n",
    "    elif reserved > 15:\n",
    "        print(\"âš¡ Good memory usage (15-20GB). Current settings are optimal.\")\n",
    "    else:\n",
    "        print(\"âœ… Low memory usage (<15GB). You could:\")\n",
    "        print(\"   - Increase batch_size to 8 for faster training\")\n",
    "        print(\"   - Increase max_seq_length to 1024 for longer sequences\")\n",
    "        print(\"   - Use a larger dataset sample\")\n",
    "        \n",
    "    # Run nvidia-smi equivalent\n",
    "    print(\"\\nFor real-time monitoring, run this command in terminal:\")\n",
    "    print(\"nvidia-smi -l 1\")\n",
    "    \n",
    "else:\n",
    "    print(\"No GPU detected. This notebook requires a CUDA-capable GPU.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 11. Additional Testing and Usage\n",
    "\n",
    "Here are some additional functions for testing the model with custom questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_response(question, model=finetuned_model, tokenizer=tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a medical response for a given question using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The medical question to answer\n",
    "        model: The fine-tuned model\n",
    "        tokenizer: The tokenizer\n",
    "        max_length (int): Maximum length of the response\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated medical response\n",
    "    \"\"\"\n",
    "    # Format the input\n",
    "    test_input = f\"Instruction: {question}\\nResponse:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Response:\" in response:\n",
    "        generated_response = response.split(\"Response:\")[-1].strip()\n",
    "    else:\n",
    "        generated_response = response\n",
    "    \n",
    "    return generated_response\n",
    "\n",
    "# Test with a few custom questions\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes heart disease?\",\n",
    "]\n",
    "\n",
    "print(\"Testing with custom questions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    response = generate_medical_response(question)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. âœ… Set up the environment with all required libraries\n",
    "2. âœ… Loaded and preprocessed the MedQuAD dataset\n",
    "3. âœ… Configured MedGemma 4B with 4-bit quantization for memory efficiency\n",
    "4. âœ… Set up LoRA fine-tuning with optimal parameters\n",
    "5. âœ… Trained the model for 1 epoch on 1000 samples\n",
    "6. âœ… Saved the fine-tuned model and LoRA adapters\n",
    "7. âœ… Tested the model with sample questions\n",
    "8. âœ… Monitored memory usage and provided optimization suggestions\n",
    "\n",
    "### Key Configuration:\n",
    "- **Model**: MedGemma 4B Multimodal with 4-bit quantization\n",
    "- **LoRA Parameters**: r=16, alpha=32, dropout=0.05\n",
    "- **Training**: Batch size=4, Gradient accumulation=4, BF16 precision\n",
    "- **Dataset**: 1000 samples from MedQuAD (instruction-response format)\n",
    "- **Memory Usage**: Optimized for RTX 4090 24GB VRAM\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale Up**: Train on the full MedQuAD dataset if memory allows\n",
    "2. **Evaluate**: Use medical QA benchmarks to assess performance\n",
    "3. **Fine-tune Further**: Adjust hyperparameters based on validation results\n",
    "4. **Deploy**: Create a simple interface for medical question answering\n",
    "5. **Safety**: Add medical disclaimer and safety checks for production use\n",
    "\n",
    "### Important Notes:\n",
    "- This model is for educational/research purposes only\n",
    "- Medical advice should always be verified by healthcare professionals\n",
    "- Consider adding evaluation metrics and validation sets for production use\n",
    "- Monitor for potential biases in medical responses\n",
    "\n",
    "### Troubleshooting:\n",
    "- If OOM errors occur, reduce `batch_size` or `max_seq_length`\n",
    "- If model access fails, ensure HuggingFace authentication\n",
    "- For better performance, consider using the full dataset and multiple epochs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
